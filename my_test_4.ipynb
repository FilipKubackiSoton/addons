{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-04 14:17:44.035877: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-04 14:17:44.035943: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import ast\n",
    "import abc\n",
    "import random\n",
    "from typing import List, Dict, Union, Tuple\n",
    "import numpy as np\n",
    "from scipy.stats import truncnorm\n",
    "import argparse\n",
    "import timeit\n",
    "from datetime import datetime\n",
    "# from tensorflow_addons.layers.nalu import NALU\n",
    "%load_ext tensorboard\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\" # disable cuda sepeed up\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1' # disable CPU wornings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Data: \n",
      "Int: \tdist \t normal\n",
      "\t\tdata >=\t -3\n",
      "\t\tmean(s)\t 0.0\n",
      "\t\tdata <\t 3\n",
      "\t\tstd \t 1.0\n",
      "Generating Data: \n",
      "Int: \tdist \t normal\n",
      "\t\tdata >=\t -3\n",
      "\t\tmean(s)\t 0.0\n",
      "\t\tdata <\t 3\n",
      "\t\tstd \t 1.0\n",
      "Generating Data: \n",
      "Int: \tdist \t normal\n",
      "\t\tdata >=\t 10\n",
      "\t\tmean(s)\t 12.5\n",
      "\t\tdata <\t 15\n",
      "\t\tstd \t 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"-o\", \"--output\", dest=\"output\", default=\"naly_syn_simple_arith\")\n",
    "parser.add_argument(\"-d\", \"--dist\", dest=\"dist\", default=\"normal\", help=\"Prob.Dist\")\n",
    "parser.add_argument(\"-p\", \"--params\",dest=\"params\" , default=\"(-3,3)\", type=ast.literal_eval)\n",
    "parser.add_argument(\"-e\", \"--ext\",dest=\"ext\" , default=\"(10,15)\", type=ast.literal_eval)\n",
    "\n",
    "parser.add_argument(\"-n\", \"--nalu\", dest=\"nalu\", default=\"nalui1\")\n",
    "parser.add_argument(\"-se\", \"--seed\", dest=\"seed\", default=42, type=int)\n",
    "parser.add_argument(\"-op\", \"--operation\", dest=\"op\", default=\"MUL\")\n",
    "\n",
    "\n",
    "\n",
    "args = parser.parse_args([])\n",
    "\n",
    "def sample(dist, params, numDim = 2, numDP = 64000):\n",
    "    data = np.zeros(shape=(numDP, numDim))\n",
    "    if dist == \"normal\":\n",
    "        intmean = (params[0] + params[1]) / 2\n",
    "        intstd = (params[1] - params[0]) / 6\n",
    "        print(\n",
    "            \"Generating Data: \\nInt: \\tdist \\t %s\\n\\t\\tdata >=\\t %s\\n\\t\\tmean(s)\\t %s\\n\\t\\tdata <\\t %s\\n\\t\\tstd \\t %s\" % (\n",
    "                dist, params[0], intmean, params[1], intstd))\n",
    "        mi, ma = (params[0] - intmean) / intstd, (params[1] - intmean) / intstd\n",
    "        data = np.reshape(truncnorm.rvs(mi, ma, intmean, intstd, size=numDim * numDP), data.shape)\n",
    "\n",
    "    elif dist == \"uniform\":\n",
    "        print(\"Generating Data: \\nInt: \\tdist \\t %s\\n\\t\\tdata >=\\t %s\\n\\t\\tdata <\\t %s\\n\\t\\t\" % (\n",
    "        dist, params[0], params[1]))\n",
    "        data = np.reshape(np.random.uniform(params[0], params[1], size=numDim * numDP), data.shape)\n",
    "    elif dist == \"exponential\":\n",
    "        data = np.random.exponential(params, size=(numDP, numDim))\n",
    "    else:\n",
    "        raise Exception(\"Unknown distribution\")\n",
    "    data = np.reshape(data, [-1])  # reshape to mix both distributions per instance!\n",
    "    np.random.shuffle(data)\n",
    "    data = np.reshape(data, (numDP, numDim))\n",
    "    return data\n",
    "\n",
    "\n",
    "def operation(op, a, b):\n",
    "    if op.lower() == \"mul\":\n",
    "        return a * b\n",
    "    if op.lower() == \"add\":\n",
    "        return a + b\n",
    "    if op.lower() == \"sub\":\n",
    "        return a - b\n",
    "    if op.lower() == \"div\":\n",
    "        return a / b\n",
    "\n",
    "random.seed(args.seed)\n",
    "tf.random.set_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "data = sample(args.dist, args.params)\n",
    "lbls = operation(args.op, data[:,0], data[:,1])\n",
    "lbls = np.reshape(lbls, newshape=(-1, 1))\n",
    "\n",
    "int_data = sample(args.dist, args.params)\n",
    "int_lbls = operation(args.op, int_data[:,0], int_data[:,1])\n",
    "int_lbls = np.reshape(int_lbls, newshape=(-1, 1))\n",
    "\n",
    "ext_data = sample(args.dist, args.ext)\n",
    "ext_lbls = operation(args.op, ext_data[:,0], ext_data[:,1])\n",
    "ext_lbls = np.reshape(ext_lbls, newshape=(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-04 14:17:51.998965: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-11-04 14:17:51.999013: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-11-04 14:17:51.999048: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (filip-HP-ProBook-440-G3): /proc/driver/nvidia/version does not exist\n",
      "2022-11-04 14:17:51.999697: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "data_dp = tf.data.Dataset.from_tensor_slices((data, lbls)).prefetch(tf.data.AUTOTUNE).batch(BATCH_SIZE)\n",
    "int_data_dp = tf.data.Dataset.from_tensor_slices((int_data, int_lbls)).prefetch(tf.data.AUTOTUNE).batch(BATCH_SIZE)\n",
    "ext_data_dp = tf.data.Dataset.from_tensor_slices((ext_data, ext_lbls)).prefetch(tf.data.AUTOTUNE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =====================================================epoch\n",
    "import tensorflow as tf\n",
    "from typeguard import typechecked\n",
    "from typing import List\n",
    "from tensorflow_addons.utils import types\n",
    "\n",
    "\n",
    "class NALURegularizer(tf.keras.regularizers.Regularizer):\n",
    "    def __init__(self, reg_coef=0.1):\n",
    "        self.reg_coef = reg_coef\n",
    "\n",
    "    def __call__(self, var: List[tf.Variable]) -> tf.Tensor:\n",
    "        return self.reg_coef * tf.reduce_mean(\n",
    "            tf.math.maximum(tf.math.minimum(-var, var) + 20, 0)\n",
    "        )\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\"reg_coef\": float(self.reg_coef)}\n",
    "\n",
    "\n",
    "# @tf.keras.utils.register_keras_serializable(package=\"Addons\")\n",
    "class NALU(tf.keras.layers.Layer):\n",
    "    r\"\"\"Neural Arithmetic Logic Units\n",
    "\n",
    "    A layer that learns addition,substraction, multiplication and division \n",
    "    in transparent way. They layer has two paths: one for addition/substration \n",
    "    and one for multiplication/division. We can inspect weights for these two\n",
    "    paths by calling `w_hat` and `m_hat` respectively. To use this layer reliably, \n",
    "    we have to delay regularization of gating varaible that switch between two paths.\n",
    "    Ithave to be done by callback as from the layer-level we keep no information about epochs.\n",
    "\n",
    "\n",
    "    See [Neural Arithmetic Logic Units](https://arxiv.org/abs/1808.00508)\n",
    "    and [Improved Neural Arithmetic Logic Unit](https://arxiv.org/abs/2003.07629) \n",
    "\n",
    "\n",
    "    Example:\n",
    "\n",
    "    >>> BATCH_SIZE, INPUT_SIZE, OUTPUT_SIZE = 16, 5, 2\n",
    "    >>> input = tf.random.uniform((BATCH_SIZE, INPUT_SIZE))\n",
    "    >>> nalu_layer = NALU(OUTPUT_SIZE)\n",
    "    >>> predict = nalu_layer(input)\n",
    "    >>> assert predict.shape == (BATCH_SIZE, OUTPUT_SIZE)\n",
    "\n",
    "    Args:\n",
    "        input_dim (int): input \n",
    "        output_dim (int): _description_\n",
    "        regularizer (types.Regularizer, optional): _description_. Defaults to NALURegularizer(reg_coef=0.05).\n",
    "        gate_as_vector (bool, optional): _description_. Defaults to True.\n",
    "        clipping (float, optional): _description_. Defaults to None.\n",
    "        force_operation (str, optional): _description_. Defaults to None.\n",
    "        weights_separation (bool, optional): _description_. Defaults to True.\n",
    "        input_gate_dependance (bool, optional): _description_. Defaults to True.\n",
    "        w_initializer (types.Initializer, optional): _description_. Defaults to tf.random_normal_initializer( mean=0.88, stddev=0.2, seed=None ).\n",
    "        m_initializer (types.Initializer, optional): _description_. Defaults to tf.random_normal_initializer( mean=0.5, stddev=0.2, seed=None ).\n",
    "        g_initializer (types.Initializer, optional): _description_. Defaults to tf.random_normal_initializer( mean=0.0, stddev=0.2, seed=None ).\n",
    "    \"\"\"\n",
    "    \n",
    "    class Superc(tf.keras.callbacks.Callback):\n",
    "        \n",
    "        def __init__(self, \n",
    "            ext_data, ext_label,\n",
    "            int_data, int_label,\n",
    "            epochs_to_reg: int = 8, \n",
    "            lt_to_reg: float = 1.0, \n",
    "            fr_param_check: int = 10000,\n",
    "            # param_check_verbose: int = 0\n",
    "            *args, **kwargs):\n",
    "\n",
    "            \n",
    "            super(NALU.Superc, self).__init__(*args, **kwargs)\n",
    "            NALU.Superc.epochs_to_reg = epochs_to_reg\n",
    "            NALU.Superc.lt_to_reg = lt_to_reg\n",
    "            NALU.Superc.fr_param_check = fr_param_check\n",
    "            self.ext_data = ext_data\n",
    "            self.ext_label = ext_label\n",
    "            self.int_data = int_data\n",
    "            self.int_label = int_label\n",
    "            # self.param_check_verbose = param_check_verbose\n",
    "\n",
    "        def on_train_begin(self, logs = None):\n",
    "            NALU.Superc.reinit_history = []\n",
    "            NALU.Superc.reinit_counter = 0\n",
    "            NALU.Superc._steps_counter = 0\n",
    "            NALU.Superc._epoch_counter = 0\n",
    "            NALU.Superc.ext_res = []\n",
    "            NALU.Superc.int_res = []\n",
    "\n",
    "            NALU.Superc.gate_counter = 0\n",
    "\n",
    "\n",
    "        # delay regularize\n",
    "        def on_epoch_end(self, epoch, logs = None):\n",
    "            NALU.Superc._epoch_counter += 1\n",
    "            if NALU.Superc._epoch_counter > NALU.Superc.epochs_to_reg:\n",
    "                NALU.Superc.gate_counter += 1\n",
    "\n",
    "\n",
    "        def on_train_batch_end(self, batch, logs = None):\n",
    "            \n",
    "            NALU.Superc._steps_counter += 1\n",
    "            \n",
    "            # record last loss\n",
    "            NALU.Superc.reinit_history.append(logs.get(\"loss\"))\n",
    "\n",
    "            # train either active or gating\n",
    "            self.model.gating.assign(NALU.Superc._steps_counter % 10 < 8)\n",
    "\n",
    "            # turn on or of ragularization depending on epoch number and last seen loss\n",
    "\n",
    "            self.model.regularize.assign(NALU.Superc._epoch_counter > NALU.Superc.epochs_to_reg and NALU.Superc.reinit_history[-1] < NALU.Superc.lt_to_reg)\n",
    "\n",
    "\n",
    "            # reinitialisation strategy\n",
    "            split_index = len(NALU.Superc.reinit_history)//2\n",
    "            if len(NALU.Superc.reinit_history)>10000 \\\n",
    "                and NALU.Superc._epoch_counter > 0 \\\n",
    "                and NALU.Superc._epoch_counter%10==1 \\\n",
    "                and tf.math.reduce_mean(NALU.Superc.reinit_history[:split_index]) <= tf.math.add(\n",
    "                            tf.math.reduce_mean(NALU.Superc.reinit_history[split_index:]),\n",
    "                            tf.math.reduce_std(NALU.Superc.reinit_history[split_index:])\n",
    "                            ) \\\n",
    "                and tf.math.reduce_mean(NALU.Superc.reinit_history[split_index:]) > 1:\n",
    "                # reinitialize all nalu layers \n",
    "                self.model.reinitialise()\n",
    "                NALU.Superc.reinit_history = []\n",
    "                NALU.Superc.reinit_counter += 1\n",
    "\n",
    "            # check parameters\n",
    "            if self._steps_counter % NALU.Superc.fr_param_check == 0:\n",
    "                # if self.param_check_verbose>1:\n",
    "                #     tf.print(\"\\n--- Parameter Check Start---\")\n",
    "                eloss_ex = self.model.compiled_loss._losses[0](self.model.predict(self.ext_data, verbose = 0), self.ext_label)\n",
    "                eloss_in = self.model.compiled_loss._losses[0](self.model.predict(self.int_data, verbose = 0), self.int_label)\n",
    "                NALU.Superc.ext_res.append(eloss_ex)\n",
    "                NALU.Superc.int_res.append(eloss_in)\n",
    "                # if self.param_check_verbose>1:\n",
    "                #     tf.print(\"--- Parameter Check End---\")\n",
    "\n",
    "\n",
    "    @typechecked\n",
    "    def __init__(\n",
    "        self,\n",
    "        output_dim: int,\n",
    "        regularizer: types.Regularizer = NALURegularizer(reg_coef=0.05),\n",
    "        gate_as_vector: bool = True,\n",
    "        clipping: float = None,\n",
    "        force_operation: str = None,\n",
    "        weights_separation: bool = True,\n",
    "        input_gate_dependance: bool = True,\n",
    "        w_initializer: types.Initializer = tf.random_normal_initializer(\n",
    "            mean=0.88, stddev=0.2, seed=None\n",
    "        ),\n",
    "        m_initializer: types.Initializer = tf.random_normal_initializer(\n",
    "            mean=0.5, stddev=0.2, seed=None\n",
    "        ),\n",
    "        g_initializer: types.Initializer = tf.random_normal_initializer(\n",
    "            mean=0.0, stddev=0.2, seed=None\n",
    "        ),\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(NALU, self).__init__(*args, **kwargs)\n",
    "\n",
    "        self.reg_fn = regularizer\n",
    "        self.output_dim = output_dim\n",
    "        self.gate_as_vector = gate_as_vector\n",
    "        self.clipping = clipping\n",
    "        self.force_operation = force_operation\n",
    "        self.weights_separation = weights_separation\n",
    "        self.input_gate_dependance = input_gate_dependance\n",
    "        self.w_initializer = w_initializer\n",
    "        self.m_initializer = m_initializer\n",
    "        self.g_initializer = g_initializer\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_dim = int(input_shape[-1])\n",
    "        if self.output_dim is None:\n",
    "            raise ValueError('The last dimension of the inputs to `NALU` '\n",
    "                            'should be defined. Found `None`.')\n",
    "\n",
    "        # if-else statement is evaluated at the graph construction time, so no need to use tf.cond\n",
    "        self.operator_weight_shape = (self.input_dim, self.output_dim) if self.gate_as_vector else (self.output_dim, self.input_dim)\n",
    "        self.gate_shape = (self.input_dim, self.output_dim if self.gate_as_vector else 1) if self.input_gate_dependance else (self.output_dim,)\n",
    "\n",
    "        # action variables\n",
    "        self.w_hat = self.add_weight(\n",
    "            name = \"w\",\n",
    "            shape = self.operator_weight_shape, \n",
    "            dtype = tf.float32,\n",
    "            initializer = self.w_initializer,\n",
    "            regularizer = self.reg_fn, \n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "        self.m_hat = self.add_weight(\n",
    "            name = \"m\",\n",
    "            shape = self.operator_weight_shape,\n",
    "            dtype = tf.float32,\n",
    "            initializer = self.m_initializer,\n",
    "            regularizer = self.reg_fn, \n",
    "            trainable=True,\n",
    "        )\n",
    "        \n",
    "        # extra weight for operation learning separation\n",
    "        if self.weights_separation:\n",
    "            self.w_hat_prime = self.add_weight(\n",
    "                name = \"w_prime\",\n",
    "                shape =self.operator_weight_shape,\n",
    "                dtype = tf.float32,\n",
    "                initializer = self.w_initializer,\n",
    "                regularizer = self.reg_fn, \n",
    "                trainable=True,\n",
    "            )\n",
    "\n",
    "            self.m_hat_prime = self.add_weight(\n",
    "                name = \"m_prime\",\n",
    "                shape = self.operator_weight_shape,\n",
    "                dtype = tf.float32,\n",
    "                initializer = self.m_initializer,\n",
    "                regularizer = self.reg_fn, \n",
    "                trainable=True,\n",
    "            )\n",
    "\n",
    "        # gating varaible\n",
    "\n",
    "        self.g = self.add_weight(\n",
    "            name = \"g\",\n",
    "            shape = self.gate_shape, \n",
    "            dtype = tf.float32,\n",
    "            initializer = self.g_initializer,\n",
    "            regularizer = self.reg_fn, \n",
    "            trainable=False,\n",
    "        )\n",
    "\n",
    "    def call(self, input):\n",
    "        eps = 1e-7\n",
    "        w1 = tf.math.tanh(self.w_hat) * tf.math.sigmoid(self.m_hat)\n",
    "        w1_prime = (\n",
    "            tf.math.tanh(self.w_hat_prime) * tf.math.sigmoid(self.m_hat_prime)\n",
    "            if self.weights_separation\n",
    "            else tf.ones((1, 1))\n",
    "        )  # dummy vector\n",
    "        w1 = w1 if self.gate_as_vector else tf.transpose(w1)\n",
    "        w1_prime = w1_prime if self.gate_as_vector else tf.transpose(w1_prime)\n",
    "        a1 = tf.matmul(input, w1)\n",
    "        m1 = tf.matmul(\n",
    "            tf.math.log(tf.maximum(tf.math.abs(input), eps)),\n",
    "            w1_prime if self.weights_separation else w1,\n",
    "        )\n",
    "        m1 = tf.math.exp(tf.minimum(m1, self.clipping) if self.clipping else m1)\n",
    "        g1 = tf.math.sigmoid(\n",
    "            tf.matmul(input, self.g) if self.input_gate_dependance else self.g\n",
    "        )\n",
    "        if self.clipping:\n",
    "            # sign correction\n",
    "            ws = tf.math.abs(\n",
    "                tf.reshape(w1_prime if self.weights_separation else w1, [-1])\n",
    "            )\n",
    "            xs = tf.concat([input] * w1.shape[1], axis=1)\n",
    "            xs = tf.reshape(xs, shape=[-1, w1.shape[0] * w1.shape[1]])\n",
    "            sgn = tf.sign(xs) * ws + (1 - ws)\n",
    "            sgn = tf.reshape(sgn, shape=[-1, w1.shape[1], w1.shape[0]])\n",
    "            ms = tf.math.reduce_prod(sgn, axis=2)\n",
    "            m1 = m1 * tf.clip_by_value(ms, -1, 1)\n",
    "\n",
    "        if not self.force_operation:\n",
    "            return g1 * a1 + (1 - g1) * m1\n",
    "        elif self.force_operation.lower() == \"add\":\n",
    "            return a1\n",
    "        elif self.force_operation.lower() == \"mul\":\n",
    "            return m1\n",
    "        else:\n",
    "            # raise ValueError(f\"force_operation must be: None/add/mul, but was: {self.force_operation}\")\n",
    "            raise tf.errors.InvalidArgumentError(\n",
    "                f\"force_operation must be: None/'add'/'mul', but was: {self.force_operation}\"\n",
    "            )\n",
    "\n",
    "    def reinitialise(self):\n",
    "        self.g.assign(tf.random.uniform(self.g.shape, -2, 2))\n",
    "        self.w_hat.assign(tf.random.uniform(self.w_hat.shape, -2, 2))\n",
    "        self.m_hat.assign(tf.random.uniform(self.m_hat.shape, -2, 2))\n",
    "        if self.weights_separation:\n",
    "            self.w_hat_prime.assign(tf.random.uniform(self.w_hat_prime.shape, -2, 2))\n",
    "            self.m_hat_prime.assign(tf.random.uniform(self.m_hat_prime.shape, -2, 2))\n",
    "\n",
    "\n",
    "    def get_gates_variables(self) -> List[tf.Variable]:\n",
    "        return [self.g]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# check if NALU layer returns expected shape in all possible configurations\n",
    "input = next(iter(data_dp))[0]\n",
    "i = 1\n",
    "for gate_as_vector in [True, False]:\n",
    "    for clipping in [None, 20]:\n",
    "        for force_operation in [None, \"add\", \"mul\"]:\n",
    "            for weights_separation in [False, True]:\n",
    "                for input_gate_dependance in [False, True]:\n",
    "                    layer = NALU(\n",
    "                        i,\n",
    "                        gate_as_vector = gate_as_vector,\n",
    "                        clipping = clipping,\n",
    "                        force_operation = force_operation,\n",
    "                        weights_separation = weights_separation,\n",
    "                        input_gate_dependance = input_gate_dependance\n",
    "                    )\n",
    "                    assert layer(input).shape == (BATCH_SIZE, i)\n",
    "                    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads_ = None\n",
    "class NALUModelSuper(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(NALUModelSuper, self).__init__(*args, **kwargs)\n",
    "        self.steps_counter = tf.Variable(0, trainable=False)\n",
    "        self.epoch_counter = tf.Variable(0, trainable=False)\n",
    "        self.reinitialization_counter = tf.Variable(0, trainable=False)\n",
    "        self.regularize = tf.Variable(False, trainable=False)\n",
    "        self.gating = tf.Variable(False, trainable=False)\n",
    "        self.gate_var = None\n",
    "\n",
    "    def reinitialise(self):\n",
    "        for l in self.layers:\n",
    "            if isinstance(l, NALU):\n",
    "                l.reinitialise()\n",
    "\n",
    "\n",
    "    def get_gates_variables(self) -> List[tf.Variable]:\n",
    "        return [l.g for l in self.layers if isinstance(l, NALU)]\n",
    "\n",
    "    def get_regularization_loss(self):\n",
    "        return tf.math.reduce_sum(self.losses)\n",
    "\n",
    "    @tf.function\n",
    "    def train_step_active(self, data):\n",
    "        x, y = data\n",
    "        with tf.GradientTape(watch_accessed_variables = True) as tape:\n",
    "            logits = self(x, training=True) \n",
    "            loss_value = self.compiled_loss(y, logits)\n",
    "            loss_value = tf.math.add(loss_value, tf.cond(\n",
    "                tf.math.logical_and(self.regularize, tf.math.less(loss_value, 1.0)),\n",
    "                lambda: self.get_regularization_loss(), \n",
    "                lambda: tf.constant(0, dtype = tf.float32)))\n",
    "        \n",
    "        grads = tape.gradient(loss_value, tape.watched_variables())\n",
    "        clipped_grads = [tf.clip_by_value(grad, -0.1, 0.1) for grad in grads]\n",
    "        self.optimizer.apply_gradients(zip(clipped_grads, tape.watched_variables()))\n",
    "        self.compiled_metrics.update_state(y, logits)\n",
    "        return  {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    @tf.function\n",
    "    def train_step_gating(self, data):\n",
    "        if not self.gate_var:\n",
    "            self.gate_var = self.get_gates_variables()\n",
    "        x, y = data\n",
    "        with tf.GradientTape(watch_accessed_variables = False) as tape:\n",
    "            for v in self.gate_var:\n",
    "                tape.watch(v)\n",
    "            logits = self(x, training=True) \n",
    "            loss_value = self.compiled_loss(y, logits)\n",
    "            loss_value = tf.math.add(loss_value, tf.cond(\n",
    "                tf.math.logical_and(self.regularize, tf.math.less(loss_value, 1.0)),\n",
    "                lambda: self.get_regularization_loss(), \n",
    "                lambda: tf.constant(0, dtype = tf.float32)))\n",
    "\n",
    "        grads = tape.gradient(loss_value, tape.watched_variables())\n",
    "        \n",
    "        clipped_grads = [tf.clip_by_value(grad, -0.1, 0.1) for grad in grads]\n",
    "        self.optimizer.apply_gradients(zip(clipped_grads, tape.watched_variables()))\n",
    "        self.compiled_metrics.update_state(y, logits)\n",
    "        return  {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, data):\n",
    "        \"\"\"\n",
    "        Specifying tf.function(input_signature=...) slows down the computation, but it leads to greater control:\n",
    "        https://www.neuralconcept.com/post/in-graph-training-loop\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        return tf.cond(self.gating, \n",
    "            lambda: self.train_step_active(data),\n",
    "            lambda: self.train_step_gating(data))\n",
    "\n",
    "g_initializer = tf.random_normal_initializer(mean=0, stddev=0.1, seed=args.seed)\n",
    "w_initializer = tf.random_normal_initializer(mean=1, stddev=0.1, seed=args.seed)\n",
    "m_initializer = tf.random_normal_initializer(mean=-1, stddev=0.1, seed=args.seed)\n",
    "\n",
    "class NALUModel(NALUModelSuper):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(NALUModel, self).__init__(*args, **kwargs)\n",
    "        self.layer = NALU(\n",
    "            1,\n",
    "            input_gate_dependance=False,\n",
    "            g_initializer = g_initializer,\n",
    "            w_initializer = w_initializer,\n",
    "            m_initializer = m_initializer,\n",
    "                    \n",
    "        )        \n",
    "\n",
    "    def call(self, inputs: tf.Tensor, training: bool = True):\n",
    "        return  self.layer(inputs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 12s 9ms/step - loss: 0.9936 - mae: 0.6387\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.9528 - mae: 0.6262\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.9507 - mae: 0.6260\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.9500 - mae: 0.6260\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.9496 - mae: 0.6260\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.9494 - mae: 0.6259\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.9493 - mae: 0.6259\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.9492 - mae: 0.6259\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.9492 - mae: 0.6259\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.9491 - mae: 0.6259\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.9491 - mae: 0.6259\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 20s 20ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 36s 36ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 11s 11ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 11s 11ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 45s 45ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 58s 58ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 11s 11ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 68s 68ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 57/100\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 58/100\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 59/100\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 60/100\n",
      "1000/1000 [==============================] - 11s 11ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 61/100\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 62/100\n",
      "1000/1000 [==============================] - 79s 79ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 63/100\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 64/100\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 65/100\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 66/100\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 67/100\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 68/100\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 69/100\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 70/100\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 71/100\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 72/100\n",
      "1000/1000 [==============================] - 89s 89ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 73/100\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 74/100\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 75/100\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 76/100\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 77/100\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 78/100\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 79/100\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 80/100\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 81/100\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 82/100\n",
      "1000/1000 [==============================] - 98s 98ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 83/100\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 84/100\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 85/100\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 86/100\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 87/100\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 88/100\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 89/100\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 90/100\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 91/100\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 92/100\n",
      "1000/1000 [==============================] - 108s 108ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 93/100\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 94/100\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 95/100\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 96/100\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 97/100\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 98/100\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 99/100\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.9490 - mae: 0.6259\n",
      "Epoch 100/100\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.9490 - mae: 0.6259\n"
     ]
    }
   ],
   "source": [
    "mm = NALUModel()\n",
    "mm.compile(optimizer=\n",
    "    tf.keras.optimizers.Adam(learning_rate=0.01), \n",
    "    loss=\"mse\", \n",
    "    metrics=[\"mae\"])\n",
    "# print(mm.variables)\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "metrtics = mm.fit(\n",
    "    data_dp,\n",
    "    epochs = epochs, \n",
    "    batch_size = batch_size,\n",
    "    callbacks=[\n",
    "        NALU.Superc(ext_data, ext_lbls,\n",
    "                    int_data, int_lbls,\n",
    "                    fr_param_check=1000,\n",
    "                    epochs_to_reg = 10\n",
    "                    )\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=() dtype=bool, numpy=False>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mm.regularize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float64, numpy=0.9598590967016019>,\n",
       " <tf.Tensor: shape=(), dtype=float64, numpy=0.9555271605432155>,\n",
       " <tf.Tensor: shape=(), dtype=float64, numpy=0.9544298118874706>,\n",
       " <tf.Tensor: shape=(), dtype=float64, numpy=0.9539517476606666>,\n",
       " <tf.Tensor: shape=(), dtype=float64, numpy=0.9536981308794836>,\n",
       " <tf.Tensor: shape=(), dtype=float64, numpy=0.9535404966915608>,\n",
       " <tf.Tensor: shape=(), dtype=float64, numpy=0.9534341284352233>,\n",
       " <tf.Tensor: shape=(), dtype=float64, numpy=0.9533581300886965>,\n",
       " <tf.Tensor: shape=(), dtype=float64, numpy=0.9533012495382589>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extd = NALU.Superc.ext_res\n",
    "intd = NALU.Superc.int_res\n",
    "intd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float64, numpy=24330.61453685744>,\n",
       " <tf.Tensor: shape=(), dtype=float64, numpy=24431.26579513374>,\n",
       " <tf.Tensor: shape=(), dtype=float64, numpy=24470.61608710688>,\n",
       " <tf.Tensor: shape=(), dtype=float64, numpy=24494.76340992634>,\n",
       " <tf.Tensor: shape=(), dtype=float64, numpy=24510.791795583584>,\n",
       " <tf.Tensor: shape=(), dtype=float64, numpy=24521.445282600733>,\n",
       " <tf.Tensor: shape=(), dtype=float64, numpy=24528.508392784453>,\n",
       " <tf.Tensor: shape=(), dtype=float64, numpy=24533.432544558324>,\n",
       " <tf.Tensor: shape=(), dtype=float64, numpy=24537.17891291054>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"int_data.csv\", \"w\") as f:\n",
    "    write = csv.writer(f)\n",
    "    for x in intd:\n",
    "        write.writerow([x.numpy()])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = args.seed\n",
    "g_intializer = tf.random_normal_initializer(mean=0, stddev=0.1, seed=seed)\n",
    "w_intializer = tf.random_normal_initializer(mean=1, stddev=0.1, seed=seed)\n",
    "m_intializer = tf.random_normal_initializer(mean=-1, stddev=0.1, seed=seed)\n",
    "batch_size = 64\n",
    "clipping = 20\n",
    "lr = 0.01\n",
    "reg_coef = 0.05\n",
    "\n",
    "tmp = None\n",
    "\n",
    "\n",
    "class Superc(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def __init__(self, \n",
    "        epochs_to_reg: int = 5, \n",
    "        lt_to_reg: float = 1.0, \n",
    "        fr_param_check: int = 1500,\n",
    "        *args, **kwargs):\n",
    "        \n",
    "        super(Superc, self).__init__(*args, **kwargs)\n",
    "        Superc.epochs_to_reg = 5\n",
    "        Superc.lt_to_reg = lt_to_reg\n",
    "        Superc.fr_param_check = fr_param_check\n",
    "    \n",
    "    def on_train_begin(self, logs = None):\n",
    "        Superc.reinit_history = []\n",
    "        Superc.reinit_counter = 0\n",
    "        Superc._steps_counter = 0\n",
    "        Superc._epoch_counter = 0\n",
    "\n",
    "    # delay regularize\n",
    "    def on_train_epoch_end(self, epoch, logs = None):\n",
    "        Superc._epoch_counter += 1\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs = None):\n",
    "        \n",
    "        Superc._steps_counter += 1\n",
    "        \n",
    "        # record last loss\n",
    "        Superc.reinit_history.append(logs.get(\"loss\"))\n",
    "\n",
    "        # train either active or gating\n",
    "        self.model.gating.assign(Superc._steps_counter % 10 < 8)\n",
    "\n",
    "        # turn on or of ragularization depending on epoch number and last seen loss\n",
    "        self.model.regularize.assign(Superc._epoch_counter > Superc.epochs_to_reg and Superc.reinit_history[-1] < Superc.lt_to_reg)\n",
    "\n",
    "\n",
    "        # reinitialisation strategy\n",
    "        split_index = len(Superc.reinit_history)//2\n",
    "        if len(Superc.reinit_history)>10000 \\\n",
    "            and Superc._epoch_counter > 0 \\\n",
    "            and Superc._epoch_counter%10==1 \\\n",
    "            and tf.math.reduce_mean(Superc.reinit_history[:split_index]) <= tf.math.add(\n",
    "                        tf.math.reduce_mean(Superc.reinit_history[split_index:]),\n",
    "                        tf.math.reduce_std(Superc.reinit_history[split_index:])\n",
    "                        ) \\\n",
    "            and tf.math.reduce_mean(Superc.reinit_history[split_index:]) > 1:\n",
    "            # reinitialize all nalu layers \n",
    "            self.model.reinitialise()\n",
    "            Superc.reinit_history = []\n",
    "            Superc.reinit_counter += 1\n",
    "\n",
    "        # check parameters\n",
    "        if self._steps_counter % Superc.fr_param_check == 0:\n",
    "            tf.print(\"\\n--- Parameter Check Start---\")\n",
    "            eloss_ex = self.model.compiled_loss._losses[0](self.model.predict(ext_data_dp, verbose = 1), ext_lbls)\n",
    "            eloss_in = self.model.compiled_loss._losses[0](self.model.predict(int_data_dp, verbose = 1), int_lbls)\n",
    "            tf.print(\"--- Parameter Check End---\")\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "     \n",
    "mm = NALUModel()\n",
    "mm.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "# print(mm.variables)\n",
    "\n",
    "metrtics = mm.fit(\n",
    "    data_dp,\n",
    "    epochs = 3, \n",
    "    batch_size = batch_size, \n",
    "    callbacks=[\n",
    "        NALU.Superc()\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# garbage collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'Superc' has no attribute 'reinit_history'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/filip/workspace/tf/addons/my_test_4.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/filip/workspace/tf/addons/my_test_4.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m NALU\u001b[39m.\u001b[39;49mSuperc\u001b[39m.\u001b[39;49mreinit_history[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'Superc' has no attribute 'reinit_history'"
     ]
    }
   ],
   "source": [
    "NALU.Superc.reinit_history[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tmp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/filip/workspace/tf/addons/my_test_4.ipynb Cell 18\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/filip/workspace/tf/addons/my_test_4.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m tmp\u001b[39m.\u001b[39mx\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tmp' is not defined"
     ]
    }
   ],
   "source": [
    "tmp.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "2000/2000 [==============================] - 5s 2ms/step\n",
      "tf.Tensor(21808.44438777578, shape=(), dtype=float64)\n",
      "Epoch: 1\n",
      "2000/2000 [==============================] - 4s 2ms/step\n",
      "tf.Tensor(22684.224431035356, shape=(), dtype=float64)\n",
      "Epoch: 2\n",
      "2000/2000 [==============================] - 4s 2ms/step\n",
      "tf.Tensor(22607.82863809394, shape=(), dtype=float64)\n",
      "Epoch: 3\n",
      "2000/2000 [==============================] - 4s 2ms/step\n",
      "tf.Tensor(22362.5936101301, shape=(), dtype=float64)\n",
      "Epoch: 4\n",
      "2000/2000 [==============================] - 4s 2ms/step\n",
      "tf.Tensor(22144.852752745664, shape=(), dtype=float64)\n",
      "Epoch: 5\n",
      "2000/2000 [==============================] - 4s 2ms/step\n",
      "tf.Tensor(21976.898584666094, shape=(), dtype=float64)\n",
      "Epoch: 6\n",
      "2000/2000 [==============================] - 4s 2ms/step\n",
      "tf.Tensor(21846.49514842637, shape=(), dtype=float64)\n",
      "Epoch: 7\n",
      "2000/2000 [==============================] - 4s 2ms/step\n",
      "tf.Tensor(21735.00620283161, shape=(), dtype=float64)\n",
      "Epoch: 8\n",
      "1000/1000 [==============================] - 2s 2ms/step\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'BatchDataset' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/filip/workspace/tf/addons/my_test_4.ipynb Cell 20\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/filip/workspace/tf/addons/my_test_4.ipynb#X23sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mif\u001b[39;00m epoch \u001b[39m>\u001b[39m \u001b[39m7\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/filip/workspace/tf/addons/my_test_4.ipynb#X23sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     good \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/filip/workspace/tf/addons/my_test_4.ipynb#X23sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     \u001b[39mfor\u001b[39;00m x, y \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(mm\u001b[39m.\u001b[39mpredict(data_dp), data_dp[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/filip/workspace/tf/addons/my_test_4.ipynb#X23sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mabs\u001b[39m(x \u001b[39m-\u001b[39m y)\u001b[39m<\u001b[39m \u001b[39m1e-5\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/filip/workspace/tf/addons/my_test_4.ipynb#X23sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m             good \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'BatchDataset' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# with open(\"%s.csv\" % \"_\".join([args.output, args.nalu, args.dist, str(args.params), str(args.ext), str(args.seed), args.op, \"int\"]), \"w\") as intlog:\n",
    "#     with open(\"%s.csv\" % \"_\".join([args.output, args.nalu, args.dist, str(args.params), str(args.ext), str(args.seed), args.op, \"ext\"]), \"w\") as extlog:\n",
    "losses = []\n",
    "reinitctr = 0\n",
    "REGULARIZATION = True\n",
    "for epoch in range(10):\n",
    "    print(\"Epoch:\",epoch)\n",
    "    for i, data in enumerate(data_dp):\n",
    "        if i % 10 < 8:\n",
    "            loss_value = mm.train_step_active(data)[\"loss\"]\n",
    "        else:\n",
    "            loss_value = mm.train_step_gating(data)[\"loss\"]\n",
    "\n",
    "        if REGULARIZATION:\n",
    "            if epoch > 10 and loss_value < 1:\n",
    "                mm.regularize.assign(True)\n",
    "                loss_value = mm.train_step_active(data)[\"loss\"]\n",
    "                loss_value = mm.train_step_gating(data)[\"loss\"]\n",
    "            else:\n",
    "                mm.regularize.assign(False)\n",
    "        losses.append(loss_value)\n",
    "\n",
    "        # reinitialisation strategy\n",
    "        split_index = len(losses)//2\n",
    "        if split_index>10000//BATCH_SIZE \\\n",
    "            and epoch > 0 \\\n",
    "            and epoch%10==1 \\\n",
    "            and tf.math.reduce_mean(losses[:split_index]) <= tf.math.add(\n",
    "                        tf.math.reduce_mean(losses[split_index:]),\n",
    "                        tf.math.reduce_std(losses[split_index:])\n",
    "                        ) \\\n",
    "            and tf.math.reduce_mean(losses[split_index:]) > 1:\n",
    "            mm.reinitialise()\n",
    "            losses = []\n",
    "            reinitctr += 1\n",
    "\n",
    "        if i%1000 == 0:\n",
    "            eloss_ex = mm.compiled_loss._losses[0](mm.predict(ext_data), ext_lbls) # np.mean([mm.test_step(x)[\"loss\"] for x in ext_data_dp])\n",
    "            print(eloss_ex)\n",
    "            # eloss_in = np.mean([mm.test_step(x)[\"loss\"] for x in int_data_dp])\n",
    "\n",
    "            # print(\"int loss: {:.5E}\\tregularization-loss: {:.5E}\".format(eloss_in, erloss_in))\n",
    "            \n",
    "            # intlog.write(\"\\t\".join([str(epoch), str(i), args.output, args.nalu, args.dist, str(args.params), str(args.ext), str(args.seed), args.op, \"{:.5E}\".format(eloss), \"{:.5E}\".format(eloss_in), str(reinitctr)])+\"\\n\")\n",
    "            # extlog.write(\"\\t\".join([str(epoch), str(i), args.output, args.nalu, args.dist, str(args.params), str(args.ext), str(args.seed), args.op, \"{:.5E}\".format(eloss), \"{:.5E}\".format(eloss_ex), str(reinitctr)]) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=bool, numpy=True>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#  PREVIOUS CALLBACK KEEP FOR INSPECTION\n",
    "\n",
    "class DelayRegularize(tf.keras.callbacks.Callback):\n",
    "    regularize_epochs = 5    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        tf.cond(tf.math.greater(epoch, DelayRegularize.regularize_epochs), lambda: self.model.regularize.assign(True), lambda: None)\n",
    "        \n",
    "\n",
    "class ReinitializatoinControll(tf.keras.callbacks.Callback):\n",
    "\n",
    "    history = []\n",
    "    counter = 0\n",
    "    \n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        ReinitializatoinControll.history.append(logs.get(\"loss\"))\n",
    "    \n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        split_index = len(ReinitializatoinControll.history)//2\n",
    "        if split_index>500 \\\n",
    "            and epoch > 0 \\\n",
    "            and epoch%10==1 \\\n",
    "            and tf.math.reduce_mean(self.history[:split_index]) <= tf.math.add(\n",
    "                        tf.math.reduce_mean(self.history[split_index:]),\n",
    "                        tf.math.reduce_std(self.history[split_index:])\n",
    "                        ) \\\n",
    "            and tf.math.reduce_mean(ReinitializatoinControll.history[split_index:]) > 1:\n",
    "            self.model.reinitialise()\n",
    "            ReinitializatoinControll.history = []\n",
    "            ReinitializatoinControll.counter += 1\n",
    "\n",
    "\n",
    "class Evaluator(tf.keras.callbacks.Callback):\n",
    "    history = []\n",
    "    def on_test_batch_end(self, epoch, logs = None):\n",
    "        if tf.math.mod(self.model.steps_counter, 10)==0:\n",
    "            Evaluator.history.append(logs.get(\"loss\"))\n",
    "\n",
    "class EXTCSV(tf.keras.callbacks.Callback):\n",
    "    counter = 0\n",
    "\n",
    "    def on_train_begin(self, logs = None):\n",
    "        pass\n",
    "    \n",
    "    def on_train_epoch_end(self, epoch, logs = None):\n",
    "        EXTCSV.counter = epoch\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs = None):\n",
    "        if EXTCSV.counter*64 + batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c6b4ff1917ad7fe489d902c3041fcac2959f3e2431a3acc02677f15f306757aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
