{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-02 12:11:06.443774: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-02 12:11:06.443823: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import ast\n",
    "import abc\n",
    "import random\n",
    "from typing import List, Dict, Union, Tuple\n",
    "import numpy as np\n",
    "from scipy.stats import truncnorm\n",
    "import argparse\n",
    "import timeit\n",
    "from datetime import datetime\n",
    "# from tensorflow_addons.layers.nalu import NALU\n",
    "%load_ext tensorboard\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\" # disable cuda sepeed up\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1' # disable CPU wornings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Data: \n",
      "Int: \tdist \t normal\n",
      "\t\tdata >=\t -3\n",
      "\t\tmean(s)\t 0.0\n",
      "\t\tdata <\t 3\n",
      "\t\tstd \t 1.0\n",
      "Generating Data: \n",
      "Int: \tdist \t normal\n",
      "\t\tdata >=\t -3\n",
      "\t\tmean(s)\t 0.0\n",
      "\t\tdata <\t 3\n",
      "\t\tstd \t 1.0\n",
      "Generating Data: \n",
      "Int: \tdist \t normal\n",
      "\t\tdata >=\t 10\n",
      "\t\tmean(s)\t 12.5\n",
      "\t\tdata <\t 15\n",
      "\t\tstd \t 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"-o\", \"--output\", dest=\"output\", default=\"naly_syn_simple_arith\")\n",
    "parser.add_argument(\"-d\", \"--dist\", dest=\"dist\", default=\"normal\", help=\"Prob.Dist\")\n",
    "parser.add_argument(\"-p\", \"--params\",dest=\"params\" , default=\"(-3,3)\", type=ast.literal_eval)\n",
    "parser.add_argument(\"-e\", \"--ext\",dest=\"ext\" , default=\"(10,15)\", type=ast.literal_eval)\n",
    "\n",
    "parser.add_argument(\"-n\", \"--nalu\", dest=\"nalu\", default=\"nalui1\")\n",
    "parser.add_argument(\"-se\", \"--seed\", dest=\"seed\", default=42, type=int)\n",
    "parser.add_argument(\"-op\", \"--operation\", dest=\"op\", default=\"MUL\")\n",
    "\n",
    "\n",
    "\n",
    "args = parser.parse_args([])\n",
    "\n",
    "def sample(dist, params, numDim = 2, numDP = 64000):\n",
    "    data = np.zeros(shape=(numDP, numDim))\n",
    "    if dist == \"normal\":\n",
    "        intmean = (params[0] + params[1]) / 2\n",
    "        intstd = (params[1] - params[0]) / 6\n",
    "        print(\n",
    "            \"Generating Data: \\nInt: \\tdist \\t %s\\n\\t\\tdata >=\\t %s\\n\\t\\tmean(s)\\t %s\\n\\t\\tdata <\\t %s\\n\\t\\tstd \\t %s\" % (\n",
    "                dist, params[0], intmean, params[1], intstd))\n",
    "        mi, ma = (params[0] - intmean) / intstd, (params[1] - intmean) / intstd\n",
    "        data = np.reshape(truncnorm.rvs(mi, ma, intmean, intstd, size=numDim * numDP), data.shape)\n",
    "\n",
    "    elif dist == \"uniform\":\n",
    "        print(\"Generating Data: \\nInt: \\tdist \\t %s\\n\\t\\tdata >=\\t %s\\n\\t\\tdata <\\t %s\\n\\t\\t\" % (\n",
    "        dist, params[0], params[1]))\n",
    "        data = np.reshape(np.random.uniform(params[0], params[1], size=numDim * numDP), data.shape)\n",
    "    elif dist == \"exponential\":\n",
    "        data = np.random.exponential(params, size=(numDP, numDim))\n",
    "    else:\n",
    "        raise Exception(\"Unknown distribution\")\n",
    "    data = np.reshape(data, [-1])  # reshape to mix both distributions per instance!\n",
    "    np.random.shuffle(data)\n",
    "    data = np.reshape(data, (numDP, numDim))\n",
    "    return data\n",
    "\n",
    "\n",
    "def operation(op, a, b):\n",
    "    if op.lower() == \"mul\":\n",
    "        return a * b\n",
    "    if op.lower() == \"add\":\n",
    "        return a + b\n",
    "    if op.lower() == \"sub\":\n",
    "        return a - b\n",
    "    if op.lower() == \"div\":\n",
    "        return a / b\n",
    "\n",
    "random.seed(args.seed)\n",
    "tf.random.set_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "data = sample(args.dist, args.params)\n",
    "lbls = operation(args.op, data[:,0], data[:,1])\n",
    "lbls = np.reshape(lbls, newshape=(-1, 1))\n",
    "\n",
    "int_data = sample(args.dist, args.params)\n",
    "int_lbls = operation(args.op, int_data[:,0], int_data[:,1])\n",
    "int_lbls = np.reshape(int_lbls, newshape=(-1, 1))\n",
    "\n",
    "ext_data = sample(args.dist, args.ext)\n",
    "ext_lbls = operation(args.op, ext_data[:,0], ext_data[:,1])\n",
    "ext_lbls = np.reshape(ext_lbls, newshape=(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-02 12:11:13.616703: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-11-02 12:11:13.616737: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-11-02 12:11:13.616764: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (filip-HP-ProBook-440-G3): /proc/driver/nvidia/version does not exist\n",
      "2022-11-02 12:11:13.617659: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "data_dp = tf.data.Dataset.from_tensor_slices((data, lbls)).prefetch(tf.data.AUTOTUNE).batch(BATCH_SIZE)\n",
    "int_data_dp = tf.data.Dataset.from_tensor_slices((int_data, int_lbls)).prefetch(tf.data.AUTOTUNE).batch(BATCH_SIZE)\n",
    "ext_data_dp = tf.data.Dataset.from_tensor_slices((ext_data, ext_lbls)).prefetch(tf.data.AUTOTUNE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =====================================================epoch\n",
    "import tensorflow as tf\n",
    "from typeguard import typechecked\n",
    "from typing import List\n",
    "from tensorflow_addons.utils import types\n",
    "\n",
    "\n",
    "class NALURegularizer(tf.keras.regularizers.Regularizer):\n",
    "    def __init__(self, reg_coef=0.1):\n",
    "        self.reg_coef = reg_coef\n",
    "\n",
    "    def __call__(self, var: List[tf.Variable]) -> tf.Tensor:\n",
    "        return self.reg_coef * tf.reduce_mean(\n",
    "            tf.math.maximum(tf.math.minimum(-var, var) + 20, 0)\n",
    "        )\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\"reg_coef\": float(self.reg_coef)}\n",
    "\n",
    "\n",
    "# @tf.keras.utils.register_keras_serializable(package=\"Addons\")\n",
    "class NALU(tf.keras.layers.Layer):\n",
    "    r\"\"\"Neural Arithmetic Logic Units\n",
    "\n",
    "    A layer that learns addition,substraction, multiplication and division \n",
    "    in transparent way. They layer has two paths: one for addition/substration \n",
    "    and one for multiplication/division. We can inspect weights for these two\n",
    "    paths by calling `w_hat` and `m_hat` respectively. To use this layer reliably, \n",
    "    we have to delay regularization of gating varaible that switch between two paths.\n",
    "    Ithave to be done by callback as from the layer-level we keep no information about epochs.\n",
    "\n",
    "\n",
    "    See [Neural Arithmetic Logic Units](https://arxiv.org/abs/1808.00508)\n",
    "    and [Improved Neural Arithmetic Logic Unit](https://arxiv.org/abs/2003.07629) \n",
    "\n",
    "\n",
    "    Example:\n",
    "\n",
    "    >>> BATCH_SIZE, INPUT_SIZE, OUTPUT_SIZE = 16, 5, 2\n",
    "    >>> input = tf.random.uniform((BATCH_SIZE, INPUT_SIZE))\n",
    "    >>> nalu_layer = NALU(OUTPUT_SIZE)\n",
    "    >>> predict = nalu_layer(input)\n",
    "    >>> assert predict.shape == (BATCH_SIZE, OUTPUT_SIZE)\n",
    "\n",
    "    Args:\n",
    "        input_dim (int): input \n",
    "        output_dim (int): _description_\n",
    "        regularizer (types.Regularizer, optional): _description_. Defaults to NALURegularizer(reg_coef=0.05).\n",
    "        gate_as_vector (bool, optional): _description_. Defaults to True.\n",
    "        clipping (float, optional): _description_. Defaults to None.\n",
    "        force_operation (str, optional): _description_. Defaults to None.\n",
    "        weights_separation (bool, optional): _description_. Defaults to True.\n",
    "        input_gate_dependance (bool, optional): _description_. Defaults to True.\n",
    "        w_initializer (types.Initializer, optional): _description_. Defaults to tf.random_normal_initializer( mean=0.88, stddev=0.2, seed=None ).\n",
    "        m_initializer (types.Initializer, optional): _description_. Defaults to tf.random_normal_initializer( mean=0.5, stddev=0.2, seed=None ).\n",
    "        g_initializer (types.Initializer, optional): _description_. Defaults to tf.random_normal_initializer( mean=0.0, stddev=0.2, seed=None ).\n",
    "    \"\"\"\n",
    "    \n",
    "    @typechecked\n",
    "    def __init__(\n",
    "        self,\n",
    "        output_dim: int,\n",
    "        regularizer: types.Regularizer = NALURegularizer(reg_coef=0.05),\n",
    "        gate_as_vector: bool = True,\n",
    "        clipping: float = None,\n",
    "        force_operation: str = None,\n",
    "        weights_separation: bool = True,\n",
    "        input_gate_dependance: bool = True,\n",
    "        w_initializer: types.Initializer = tf.random_normal_initializer(\n",
    "            mean=0.88, stddev=0.2, seed=None\n",
    "        ),\n",
    "        m_initializer: types.Initializer = tf.random_normal_initializer(\n",
    "            mean=0.5, stddev=0.2, seed=None\n",
    "        ),\n",
    "        g_initializer: types.Initializer = tf.random_normal_initializer(\n",
    "            mean=0.0, stddev=0.2, seed=None\n",
    "        ),\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(NALU, self).__init__(*args, **kwargs)\n",
    "\n",
    "        self.reg_fn = regularizer\n",
    "        self.output_dim = output_dim\n",
    "        self.gate_as_vector = gate_as_vector\n",
    "        self.clipping = clipping\n",
    "        self.force_operation = force_operation\n",
    "        self.weights_separation = weights_separation\n",
    "        self.input_gate_dependance = input_gate_dependance\n",
    "        self.w_initializer = w_initializer\n",
    "        self.m_initializer = m_initializer\n",
    "        self.g_initializer = g_initializer\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_dim = int(input_shape[-1])\n",
    "        if self.output_dim is None:\n",
    "            raise ValueError('The last dimension of the inputs to `NALU` '\n",
    "                            'should be defined. Found `None`.')\n",
    "\n",
    "        # if-else statement is evaluated at the graph construction time, so no need to use tf.cond\n",
    "        self.operator_weight_shape = (self.input_dim, self.output_dim) if self.gate_as_vector else (self.output_dim, self.input_dim)\n",
    "        self.gate_shape = (self.input_dim, self.output_dim if self.gate_as_vector else 1) if self.input_gate_dependance else (self.output_dim,)\n",
    "\n",
    "        # action variables\n",
    "        self.w_hat = self.add_weight(\n",
    "            name = \"w\",\n",
    "            shape = self.operator_weight_shape, \n",
    "            dtype = tf.float32,\n",
    "            initializer = self.w_initializer,\n",
    "            regularizer = self.reg_fn, \n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "        self.m_hat = self.add_weight(\n",
    "            name = \"m\",\n",
    "            shape = self.operator_weight_shape,\n",
    "            dtype = tf.float32,\n",
    "            initializer = self.m_initializer,\n",
    "            regularizer = self.reg_fn, \n",
    "            trainable=True,\n",
    "        )\n",
    "        \n",
    "        # extra weight for operation learning separation\n",
    "        if self.weights_separation:\n",
    "            self.w_hat_prime = self.add_weight(\n",
    "                name = \"w_prime\",\n",
    "                shape =self.operator_weight_shape,\n",
    "                dtype = tf.float32,\n",
    "                initializer = self.w_initializer,\n",
    "                regularizer = self.reg_fn, \n",
    "                trainable=True,\n",
    "            )\n",
    "\n",
    "            self.m_hat_prime = self.add_weight(\n",
    "                name = \"m_prime\",\n",
    "                shape = self.operator_weight_shape,\n",
    "                dtype = tf.float32,\n",
    "                initializer = self.m_initializer,\n",
    "                regularizer = self.reg_fn, \n",
    "                trainable=True,\n",
    "            )\n",
    "\n",
    "        # gating varaible\n",
    "\n",
    "        self.g = self.add_weight(\n",
    "            name = \"g\",\n",
    "            shape = self.gate_shape, \n",
    "            dtype = tf.float32,\n",
    "            initializer = self.g_initializer,\n",
    "            regularizer = self.reg_fn, \n",
    "            trainable=False,\n",
    "        )\n",
    "\n",
    "    def call(self, input):\n",
    "        eps = 1e-7\n",
    "        w1 = tf.math.tanh(self.w_hat) * tf.math.sigmoid(self.m_hat)\n",
    "        w1_prime = (\n",
    "            tf.math.tanh(self.w_hat_prime) * tf.math.sigmoid(self.m_hat_prime)\n",
    "            if self.weights_separation\n",
    "            else tf.ones((1, 1))\n",
    "        )  # dummy vector\n",
    "        w1 = w1 if self.gate_as_vector else tf.transpose(w1)\n",
    "        w1_prime = w1_prime if self.gate_as_vector else tf.transpose(w1_prime)\n",
    "        a1 = tf.matmul(input, w1)\n",
    "        m1 = tf.matmul(\n",
    "            tf.math.log(tf.math.abs(input) + eps),\n",
    "            w1_prime if self.weights_separation else w1,\n",
    "        )\n",
    "        m1 = tf.math.exp(tf.minimum(m1, self.clipping) if self.clipping else m1)\n",
    "        g1 = tf.math.sigmoid(\n",
    "            tf.matmul(input, self.g) if self.input_gate_dependance else self.g\n",
    "        )\n",
    "        if self.clipping:\n",
    "            # sign correction\n",
    "            ws = tf.math.abs(\n",
    "                tf.reshape(w1_prime if self.weights_separation else w1, [-1])\n",
    "            )\n",
    "            xs = tf.concat([input] * w1.shape[1], axis=1)\n",
    "            xs = tf.reshape(xs, shape=[-1, w1.shape[0] * w1.shape[1]])\n",
    "            sgn = tf.sign(xs) * ws + (1 - ws)\n",
    "            sgn = tf.reshape(sgn, shape=[-1, w1.shape[1], w1.shape[0]])\n",
    "            ms = tf.math.reduce_prod(sgn, axis=2)\n",
    "            m1 = m1 * tf.clip_by_value(ms, -0.1, 0.1)\n",
    "\n",
    "        if not self.force_operation:\n",
    "            return g1 * a1 + (1 - g1) * m1\n",
    "        elif self.force_operation.lower() == \"add\":\n",
    "            return a1\n",
    "        elif self.force_operation.lower() == \"mul\":\n",
    "            return m1\n",
    "        else:\n",
    "            # raise ValueError(f\"force_operation must be: None/add/mul, but was: {self.force_operation}\")\n",
    "            raise tf.errors.InvalidArgumentError(\n",
    "                f\"force_operation must be: None/'add'/'mul', but was: {self.force_operation}\"\n",
    "            )\n",
    "\n",
    "    def reinitialise(self):\n",
    "        self.g.assign(tf.random.uniform(self.g.shape, -2, 2))\n",
    "        self.w_hat.assign(tf.random.uniform(self.w_hat.shape, -2, 2))\n",
    "        self.m_hat.assign(tf.random.uniform(self.m_hat.shape, -2, 2))\n",
    "        if self.weights_separation:\n",
    "            self.w_hat_prime.assign(tf.random.uniform(self.w_hat_prime.shape, -2, 2))\n",
    "            self.m_hat_prime.assign(tf.random.uniform(self.m_hat_prime.shape, -2, 2))\n",
    "\n",
    "\n",
    "    def get_gates_variables(self) -> List[tf.Variable]:\n",
    "        return [self.g]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# check if NALU layer returns expected shape in all possible configurations\n",
    "input = next(iter(data_dp))[0]\n",
    "i = 1\n",
    "for gate_as_vector in [True, False]:\n",
    "    for clipping in [None, 20]:\n",
    "        for force_operation in [None, \"add\", \"mul\"]:\n",
    "            for weights_separation in [False, True]:\n",
    "                for input_gate_dependance in [False, True]:\n",
    "                    layer = NALU(\n",
    "                        i,\n",
    "                        gate_as_vector = gate_as_vector,\n",
    "                        clipping = clipping,\n",
    "                        force_operation = force_operation,\n",
    "                        weights_separation = weights_separation,\n",
    "                        input_gate_dependance = input_gate_dependance\n",
    "                    )\n",
    "                    assert layer(input).shape == (BATCH_SIZE, i)\n",
    "                    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NALUModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(NALUModel, self).__init__(*args, **kwargs)\n",
    "        self.layer = NALU(1)        \n",
    "\n",
    "        self.steps_counter = tf.Variable(0, trainable=False)\n",
    "        self.epoch_counter = tf.Variable(0, trainable=False)\n",
    "        self.reinitialization_counter = tf.Variable(0, trainable=False)\n",
    "        self.regularize = tf.Variable(False, trainable=False)\n",
    "        self.gating = tf.Variable(False, trainable=False)\n",
    "        self.gate_var = None\n",
    "\n",
    "    def reinitialise(self):\n",
    "        for l in self.layers:\n",
    "            if isinstance(l, NALU):\n",
    "                l.reinitialise()\n",
    "\n",
    "\n",
    "    def get_gates_variables(self) -> List[tf.Variable]:\n",
    "        return [l.g for l in self.layers if isinstance(l, NALU)]\n",
    "\n",
    "    def get_regularization_loss(self):\n",
    "        return tf.math.reduce_sum(self.losses)\n",
    "\n",
    "    @tf.function\n",
    "    def train_step_active(self, data):\n",
    "        x, y = data\n",
    "        with tf.GradientTape(watch_accessed_variables = True) as tape:\n",
    "            logits = self(x, training=True) \n",
    "            loss_value = self.compiled_loss(y, logits)\n",
    "            loss_value = tf.math.add(loss_value, tf.cond(\n",
    "                tf.math.logical_and(self.regularize, tf.math.less(loss_value, 1.0)),\n",
    "                lambda: self.get_regularization_loss(), \n",
    "                lambda: tf.constant(0, dtype = tf.float32)))\n",
    "        \n",
    "        grads = tape.gradient(loss_value, tape.watched_variables())\n",
    "        self.optimizer.apply_gradients(zip(grads, tape.watched_variables()))\n",
    "        self.compiled_metrics.update_state(y, logits)\n",
    "        return  {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    @tf.function\n",
    "    def train_step_gating(self, data):\n",
    "        if not self.gate_var:\n",
    "            self.gate_var = self.get_gates_variables()\n",
    "\n",
    "        x, y = data\n",
    "        with tf.GradientTape(watch_accessed_variables = False) as tape:\n",
    "            for v in self.gate_var:\n",
    "                tape.watch(v)\n",
    "            logits = self(x, training=True) \n",
    "            loss_value = self.compiled_loss(y, logits)\n",
    "            loss_value = tf.math.add(loss_value, tf.cond(\n",
    "                tf.math.logical_and(self.regularize, tf.math.less(loss_value, 1.0)),\n",
    "                lambda: self.get_regularization_loss(), \n",
    "                lambda: tf.constant(0, dtype = tf.float32)))\n",
    "\n",
    "        grads = tape.gradient(loss_value, tape.watched_variables())\n",
    "        self.optimizer.apply_gradients(zip(grads, tape.watched_variables()))\n",
    "        self.compiled_metrics.update_state(y, logits)\n",
    "        return  {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def call(self, inputs: tf.Tensor, training: bool = True):\n",
    "        return  self.layer(inputs) # self.layer3(self.layer2(self.layer(inputs)))\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, data):\n",
    "        \"\"\"\n",
    "        Specifying tf.function(input_signature=...) slows down the computation, but it leads to greater control:\n",
    "        https://www.neuralconcept.com/post/in-graph-training-loop\n",
    "        \n",
    "        \"\"\"\n",
    "        print(\"tracking check\")\n",
    "        \n",
    "        return tf.cond(self.gating, \n",
    "            lambda: self.train_step_active(data),\n",
    "            lambda: self.train_step_gating(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = args.seed\n",
    "g_intializer = tf.random_normal_initializer(mean=0, stddev=0.1, seed=seed)\n",
    "w_intializer = tf.random_normal_initializer(mean=1, stddev=0.1, seed=seed)\n",
    "m_intializer = tf.random_normal_initializer(mean=-1, stddev=0.1, seed=seed)\n",
    "batch_size = 64\n",
    "clipping = 20\n",
    "lr = 0.01\n",
    "reg_coef = 0.05\n",
    "\n",
    "tmp = None\n",
    "\n",
    "\n",
    "class Superc(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    reinit_history = []\n",
    "    reinit_counter = 0\n",
    "    _steps_counter = 0\n",
    "    _epoch_counter = 0\n",
    "    epochs_to_reg = 5\n",
    "    lt_to_reg = 1.0\n",
    "    fr_param_check = 5\n",
    "\n",
    "    # inspect why __init__ does not work for callback subclass\n",
    "\n",
    "    # def __init_(self, epochs_to_reg: int = 5, lt_to_reg: float = 1.0, fr_param_check: int = 5, *args, **kwargs):\n",
    "    #     super(Superc, self).__init__(*args, *kwargs)\n",
    "    #     Superc.epochs_to_reg = 5\n",
    "    #     Superc.lt_to_reg = lt_to_reg\n",
    "    #     Superc.fr_param_check = fr_param_check\n",
    "\n",
    "    \n",
    "    # delay regularize\n",
    "    def on_train_epoch_end(self, epoch, logs = None):\n",
    "        Superc._epoch_counter += 1\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs = None):\n",
    "        \n",
    "        Superc._steps_counter += 1\n",
    "        \n",
    "        # record last loss\n",
    "        Superc.reinit_history.append(logs.get(\"loss\"))\n",
    "\n",
    "        # train either active or gating\n",
    "        self.model.gating.assign(Superc._steps_counter % 10 < 8)\n",
    "\n",
    "        # turn on or of ragularization depending on epoch number and last seen loss\n",
    "        self.model.regularize.assign(Superc._epoch_counter > Superc.epochs_to_reg and Superc.reinit_history[-1] < Superc.lt_to_reg)\n",
    "\n",
    "\n",
    "        # reinitialisation strategy\n",
    "        split_index = len(Superc.reinit_history)//2\n",
    "        if len(Superc.reinit_history)>10000 \\\n",
    "            and Superc._epoch_counter > 0 \\\n",
    "            and Superc._epoch_counter%10==1 \\\n",
    "            and tf.math.reduce_mean(Superc.reinit_history[:split_index]) <= tf.math.add(\n",
    "                        tf.math.reduce_mean(Superc.reinit_history[split_index:]),\n",
    "                        tf.math.reduce_std(Superc.reinit_history[split_index:])\n",
    "                        ) \\\n",
    "            and tf.math.reduce_mean(Superc.reinit_history[split_index:]) > 1:\n",
    "            # reinitialize all nalu layers \n",
    "            self.model.reinitialise()\n",
    "            Superc.reinit_history = []\n",
    "            Superc.reinit_counter += 1\n",
    "\n",
    "        # check parameters\n",
    "        if self._steps_counter % 1500 ==0:\n",
    "            tf.print(\"\\n--- Parameter Check Start---\")\n",
    "            eloss_ex = self.model.compiled_loss._losses[0](self.model.predict(ext_data_dp, verbose = 1), ext_lbls)\n",
    "            eloss_in = self.model.compiled_loss._losses[0](self.model.predict(int_data_dp, verbose = 1), int_lbls)\n",
    "            tf.print(\"--- Parameter Check End---\")\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "tracking check\n",
      "1000/1000 [==============================] - 6s 3ms/step - loss: 1.1165 - mae: 0.6848\n",
      "Epoch 2/3\n",
      " 496/1000 [=============>................] - ETA: 1s - loss: 1.0637 - mae: 0.6519\n",
      "--- Parameter Check Start---\n",
      "1000/1000 [==============================] - 2s 2ms/step\n",
      "1000/1000 [==============================] - 2s 2ms/step\n",
      "--- Parameter Check End---\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.0751 - mae: 0.6538\n",
      "Epoch 3/3\n",
      " 995/1000 [============================>.] - ETA: 0s - loss: 1.0621 - mae: 0.6462\n",
      "--- Parameter Check Start---\n",
      "1000/1000 [==============================] - 2s 2ms/step\n",
      "1000/1000 [==============================] - 2s 2ms/step\n",
      "--- Parameter Check End---\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.0616 - mae: 0.6460\n"
     ]
    }
   ],
   "source": [
    "     \n",
    "mm = NALUModel()\n",
    "mm.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "# print(mm.variables)\n",
    "\n",
    "metrtics = mm.fit(\n",
    "    data_dp,\n",
    "    epochs = 3, \n",
    "    batch_size = batch_size, \n",
    "    callbacks=[\n",
    "        Superc()\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# garbage collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=bool, numpy=True>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with open(\"%s.csv\" % \"_\".join([args.output, args.nalu, args.dist, str(args.params), str(args.ext), str(args.seed), args.op, \"int\"]), \"w\") as intlog:\n",
    "#     with open(\"%s.csv\" % \"_\".join([args.output, args.nalu, args.dist, str(args.params), str(args.ext), str(args.seed), args.op, \"ext\"]), \"w\") as extlog:\n",
    "losses = []\n",
    "reinitctr = 0\n",
    "for epoch in range(10):\n",
    "    print(\"Epoch:\",epoch)\n",
    "    for i, data in enumerate(data_dp):\n",
    "        if i % 10 < 8:\n",
    "            loss_value = mm.train_step_active(data)[\"loss\"]\n",
    "        else:\n",
    "            loss_value = mm.train_step_gating(data)[\"loss\"]\n",
    "\n",
    "        if REGULARIZATION:\n",
    "            if epoch > 10 and loss_value < 1:\n",
    "                mm.regularize.assign(True)\n",
    "                loss_value = mm.train_step_active(data)[\"loss\"]\n",
    "                loss_value = mm.train_step_gating(data)[\"loss\"]\n",
    "            else:\n",
    "                mm.regularize.assign(False)\n",
    "        losses.append(loss_value)\n",
    "\n",
    "        # reinitialisation strategy\n",
    "        split_index = len(losses)//2\n",
    "        if split_index>10000//BATCH_SIZE \\\n",
    "            and epoch > 0 \\\n",
    "            and epoch%10==1 \\\n",
    "            and tf.math.reduce_mean(losses[:split_index]) <= tf.math.add(\n",
    "                        tf.math.reduce_mean(losses[split_index:]),\n",
    "                        tf.math.reduce_std(losses[split_index:])\n",
    "                        ) \\\n",
    "            and tf.math.reduce_mean(losses[split_index:]) > 1:\n",
    "            mm.reinitialise()\n",
    "            losses = []\n",
    "            reinitctr += 1\n",
    "\n",
    "        if epoch > 5:\n",
    "            if epoch > 7:\n",
    "                good = 0\n",
    "                for x, y in zip(mm.predict(data_dp), data_dp[-1]):\n",
    "                    if abs(x - y)< 1e-5:\n",
    "                        good += 1\n",
    "                print(f\"good: {good}\")\n",
    "            eloss_ex = mm.compiled_loss(mm.predict, ext_data_dp[-1]) # np.mean([mm.test_step(x)[\"loss\"] for x in ext_data_dp])\n",
    "            print(eloss_ex)\n",
    "            # eloss_in = np.mean([mm.test_step(x)[\"loss\"] for x in int_data_dp])\n",
    "\n",
    "            # print(\"int loss: {:.5E}\\tregularization-loss: {:.5E}\".format(eloss_in, erloss_in))\n",
    "            \n",
    "            # intlog.write(\"\\t\".join([str(epoch), str(i), args.output, args.nalu, args.dist, str(args.params), str(args.ext), str(args.seed), args.op, \"{:.5E}\".format(eloss), \"{:.5E}\".format(eloss_in), str(reinitctr)])+\"\\n\")\n",
    "            # extlog.write(\"\\t\".join([str(epoch), str(i), args.output, args.nalu, args.dist, str(args.params), str(args.ext), str(args.seed), args.op, \"{:.5E}\".format(eloss), \"{:.5E}\".format(eloss_ex), str(reinitctr)]) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=bool, numpy=True>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#  PREVIOUS CALLBACK KEEP FOR INSPECTION\n",
    "\n",
    "class DelayRegularize(tf.keras.callbacks.Callback):\n",
    "    regularize_epochs = 5    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        tf.cond(tf.math.greater(epoch, DelayRegularize.regularize_epochs), lambda: self.model.regularize.assign(True), lambda: None)\n",
    "        \n",
    "\n",
    "class ReinitializatoinControll(tf.keras.callbacks.Callback):\n",
    "\n",
    "    history = []\n",
    "    counter = 0\n",
    "    \n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        ReinitializatoinControll.history.append(logs.get(\"loss\"))\n",
    "    \n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        split_index = len(ReinitializatoinControll.history)//2\n",
    "        if split_index>500 \\\n",
    "            and epoch > 0 \\\n",
    "            and epoch%10==1 \\\n",
    "            and tf.math.reduce_mean(self.history[:split_index]) <= tf.math.add(\n",
    "                        tf.math.reduce_mean(self.history[split_index:]),\n",
    "                        tf.math.reduce_std(self.history[split_index:])\n",
    "                        ) \\\n",
    "            and tf.math.reduce_mean(ReinitializatoinControll.history[split_index:]) > 1:\n",
    "            self.model.reinitialise()\n",
    "            ReinitializatoinControll.history = []\n",
    "            ReinitializatoinControll.counter += 1\n",
    "\n",
    "\n",
    "class Evaluator(tf.keras.callbacks.Callback):\n",
    "    history = []\n",
    "    def on_test_batch_end(self, epoch, logs = None):\n",
    "        if tf.math.mod(self.model.steps_counter, 10)==0:\n",
    "            Evaluator.history.append(logs.get(\"loss\"))\n",
    "\n",
    "class EXTCSV(tf.keras.callbacks.Callback):\n",
    "    counter = 0\n",
    "\n",
    "    def on_train_begin(self, logs = None):\n",
    "        pass\n",
    "    \n",
    "    def on_train_epoch_end(self, epoch, logs = None):\n",
    "        EXTCSV.counter = epoch\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs = None):\n",
    "        if EXTCSV.counter*64 + batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c6b4ff1917ad7fe489d902c3041fcac2959f3e2431a3acc02677f15f306757aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
