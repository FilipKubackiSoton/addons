{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-23 23:47:46.936945: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-10-23 23:47:46.936973: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow_addons.layers import inalu\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import ast\n",
    "import abc\n",
    "import random\n",
    "from typing import List, Dict, Union, Tuple\n",
    "import numpy as np\n",
    "from scipy.stats import truncnorm\n",
    "import argparse\n",
    "import timeit\n",
    "from datetime import datetime\n",
    "%load_ext tensorboard\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\" # disable cuda sepeed up\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1' # disable CPU wornings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Data: \n",
      "Int: \tdist \t normal\n",
      "\t\tdata >=\t -3\n",
      "\t\tmean(s)\t 0.0\n",
      "\t\tdata <\t 3\n",
      "\t\tstd \t 1.0\n",
      "Generating Data: \n",
      "Int: \tdist \t normal\n",
      "\t\tdata >=\t -3\n",
      "\t\tmean(s)\t 0.0\n",
      "\t\tdata <\t 3\n",
      "\t\tstd \t 1.0\n",
      "Generating Data: \n",
      "Int: \tdist \t normal\n",
      "\t\tdata >=\t 10\n",
      "\t\tmean(s)\t 12.5\n",
      "\t\tdata <\t 15\n",
      "\t\tstd \t 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"-o\", \"--output\", dest=\"output\", default=\"naly_syn_simple_arith\")\n",
    "parser.add_argument(\"-d\", \"--dist\", dest=\"dist\", default=\"normal\", help=\"Prob.Dist\")\n",
    "parser.add_argument(\"-p\", \"--params\",dest=\"params\" , default=\"(-3,3)\", type=ast.literal_eval)\n",
    "parser.add_argument(\"-e\", \"--ext\",dest=\"ext\" , default=\"(10,15)\", type=ast.literal_eval)\n",
    "\n",
    "parser.add_argument(\"-n\", \"--nalu\", dest=\"nalu\", default=\"nalui1\")\n",
    "parser.add_argument(\"-se\", \"--seed\", dest=\"seed\", default=42, type=int)\n",
    "parser.add_argument(\"-op\", \"--operation\", dest=\"op\", default=\"MUL\")\n",
    "\n",
    "\n",
    "\n",
    "args = parser.parse_args([])\n",
    "\n",
    "\n",
    "def sample(dist, params, numDim = 2, numDP = 64000):\n",
    "    data = np.zeros(shape=(numDP, numDim))\n",
    "    if dist == \"normal\":\n",
    "        intmean = (params[0] + params[1]) / 2\n",
    "        intstd = (params[1] - params[0]) / 6\n",
    "        print(\n",
    "            \"Generating Data: \\nInt: \\tdist \\t %s\\n\\t\\tdata >=\\t %s\\n\\t\\tmean(s)\\t %s\\n\\t\\tdata <\\t %s\\n\\t\\tstd \\t %s\" % (\n",
    "                dist, params[0], intmean, params[1], intstd))\n",
    "        mi, ma = (params[0] - intmean) / intstd, (params[1] - intmean) / intstd\n",
    "        data = np.reshape(truncnorm.rvs(mi, ma, intmean, intstd, size=numDim * numDP), data.shape)\n",
    "\n",
    "    elif dist == \"uniform\":\n",
    "        print(\"Generating Data: \\nInt: \\tdist \\t %s\\n\\t\\tdata >=\\t %s\\n\\t\\tdata <\\t %s\\n\\t\\t\" % (\n",
    "        dist, params[0], params[1]))\n",
    "        data = np.reshape(np.random.uniform(params[0], params[1], size=numDim * numDP), data.shape)\n",
    "    elif dist == \"exponential\":\n",
    "        data = np.random.exponential(params, size=(numDP, numDim))\n",
    "    else:\n",
    "        raise Exception(\"Unknown distribution\")\n",
    "    data = np.reshape(data, [-1])  # reshape to mix both distributions per instance!\n",
    "    np.random.shuffle(data)\n",
    "    data = np.reshape(data, (numDP, numDim))\n",
    "    return data\n",
    "\n",
    "\n",
    "def operation(op, a, b):\n",
    "    if op.lower() == \"mul\":\n",
    "        return a * b\n",
    "    if op.lower() == \"add\":\n",
    "        return a + b\n",
    "    if op.lower() == \"sub\":\n",
    "        return a - b\n",
    "    if op.lower() == \"div\":\n",
    "        return a / b\n",
    "\n",
    "random.seed(args.seed)\n",
    "tf.random.set_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "data = sample(args.dist, args.params)\n",
    "lbls = operation(args.op, data[:,0], data[:,1])\n",
    "lbls = np.reshape(lbls, newshape=(-1, 1))\n",
    "\n",
    "int_data = sample(args.dist, args.params)\n",
    "int_lbls = operation(args.op, int_data[:,0], int_data[:,1])\n",
    "int_lbls = np.reshape(int_lbls, newshape=(-1, 1))\n",
    "\n",
    "ext_data = sample(args.dist, args.ext)\n",
    "ext_lbls = operation(args.op, ext_data[:,0], ext_data[:,1])\n",
    "ext_lbls = np.reshape(ext_lbls, newshape=(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-23 23:48:11.044704: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-10-23 23:48:11.044769: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-10-23 23:48:11.044808: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (filip-HP-ProBook-440-G3): /proc/driver/nvidia/version does not exist\n",
      "2022-10-23 23:48:11.045266: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "data_dp = tf.data.Dataset.from_tensor_slices((data, lbls)).prefetch(tf.data.AUTOTUNE).batch(BATCH_SIZE)\n",
    "int_data_dp = tf.data.Dataset.from_tensor_slices((int_data, int_lbls)).prefetch(tf.data.AUTOTUNE).batch(BATCH_SIZE)\n",
    "ext_data_dp = tf.data.Dataset.from_tensor_slices((ext_data, ext_lbls)).prefetch(tf.data.AUTOTUNE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from abc import abstractmethod\n",
    "\n",
    "\n",
    "\n",
    "class NALURegularizer(tf.keras.regularizers.Regularizer):\n",
    "  def __init__(self, reg_coef=0.1):\n",
    "    self.reg_coef = reg_coef\n",
    "\n",
    "  def __call__(self, var: List[tf.Variable]) -> tf.Tensor:\n",
    "    return self.reg_coef * tf.reduce_mean(tf.math.maximum(tf.math.minimum(-var, var) + 20, 0))\n",
    "\n",
    "  def get_config(self):\n",
    "    return {'reg_coef': float(self.reg_coef)}\n",
    "\n",
    "\n",
    "class NALUInterface(metaclass=abc.ABCMeta):\n",
    "    @classmethod\n",
    "    def __subclasshook__(cls, subclass):\n",
    "        return (\n",
    "            hasattr(subclass, 'get_gates_variables') and\n",
    "            callable(subclass.get_gates_variables) or\n",
    "            NotImplemented)\n",
    "    \n",
    "    @abc.abstractclassmethod\n",
    "    def get_gates_variables(self) -> List[tf.Variable]:\n",
    "        \"\"\"Return list of tf gating variables\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "class NALUModelSuperClass(NALUInterface):\n",
    "    \n",
    "    steps_counter = tf.Variable(0, trainable=False)\n",
    "    epoch_counter = tf.Variable(0, trainable=False)\n",
    "    reinitialization_counter = tf.Variable(0, trainable=False)\n",
    "    regularize = tf.Variable(False, trainable=False)\n",
    "\n",
    "    def get_gates_variables(self) -> List[tf.Variable]:\n",
    "        return [v for l in self.layers if isinstance(l, NALUInterface) for v in l.get_gates_variables()]\n",
    "\n",
    "    def get_regularization_loss(self):\n",
    "        return tf.math.reduce_sum(self.losses)\n",
    "\n",
    "    @tf.function\n",
    "    def train_step_active(self, data):\n",
    "        x, y = data\n",
    "        with tf.GradientTape(watch_accessed_variables = True) as tape:\n",
    "            logits = self(x, training=True) \n",
    "            loss_value = self.compiled_loss(y, logits)\n",
    "            tf.math.add(loss_value, tf.cond(\n",
    "                tf.math.logical_and(self.regularize, tf.math.less(loss_value, 1.0)),\n",
    "                lambda: self.get_regularization_loss(), \n",
    "                lambda: tf.constant(0, dtype = tf.float32)))\n",
    "        \n",
    "        grads = tape.gradient(loss_value, tape.watched_variables())\n",
    "        self.optimizer.apply_gradients(zip(grads, tape.watched_variables()))\n",
    "        self.compiled_metrics.update_state(y, logits)\n",
    "        return  {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    @tf.function\n",
    "    def train_step_gating(self, data):\n",
    "        x, y = data\n",
    "        with tf.GradientTape(watch_accessed_variables = False) as tape:\n",
    "            for v in self.gate_var:\n",
    "                tape.watch(v)\n",
    "            logits = self(x, training=True) \n",
    "            loss_value = self.compiled_loss(y, logits)\n",
    "            tf.math.add(loss_value, tf.cond(\n",
    "                tf.math.logical_and(self.regularize, tf.math.less(loss_value, 1.0)),\n",
    "                lambda: self.get_regularization_loss(), \n",
    "                lambda: tf.constant(0, dtype = tf.float32)))\n",
    "\n",
    "        grads = tape.gradient(loss_value, tape.watched_variables())\n",
    "        self.optimizer.apply_gradients(zip(grads, tape.watched_variables()))\n",
    "        self.compiled_metrics.update_state(y, logits)\n",
    "        return  {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "\n",
    "\n",
    "class NALUModel(tf.keras.Model, NALUModelSuperClass, NALUInterface):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(NALUModel, self).__init__(*args, **kwargs)\n",
    "\n",
    "        self.layer = inalu.NALU(2,1)\n",
    "        self.layer2 = inalu.NALU(1,1) #tf.keras.layers.Dense(1)\n",
    "        self.layer3 = tf.keras.layers.Dense(1, kernel_regularizer=\"l2\")\n",
    "        self.gate_var = self.get_gates_variables()\n",
    "\n",
    "\n",
    "    def call(self, inputs: tf.Tensor, training: bool = True):\n",
    "        return self.layer3(self.layer2(self.layer(inputs)))\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, data):\n",
    "        \"\"\"\n",
    "        Specifying tf.function(input_signature=...) slows down the computation, but it leads to greater control:\n",
    "        https://www.neuralconcept.com/post/in-graph-training-loop\n",
    "        \n",
    "        \"\"\"\n",
    "        print(\"tracking check\")\n",
    "        \n",
    "        return tf.cond(\n",
    "            tf.math.greater(self.steps_counter.assign_add(1), 2), \n",
    "            lambda: self.train_step_active(data), \n",
    "            lambda: self.train_step_gating(data))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "tracking check\n",
      "1000/1000 [==============================] - 6s 3ms/step - loss: 0.9506 - mae: 0.6268\n",
      "Epoch 2/3\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.8483 - mae: 0.5821\n",
      "Epoch 3/3\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.7302 - mae: 0.5523\n"
     ]
    }
   ],
   "source": [
    "mm = NALUModel()\n",
    "mm.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "\n",
    "class DelayRegularize(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        regularize_epochs = 10\n",
    "        tf.cond(tf.math.greater(self.model.epoch_counter.assign_add(1),regularize_epochs), lambda: self.model.regularize.assign(True), lambda: None)\n",
    "        \n",
    "metrtics = mm.fit(data_dp, epochs = 3, callbacks=[DelayRegularize()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=0.87698233>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.86694765>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.99432087>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9172727>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9391996>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9996104>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.008183975>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mm.losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "logdir = 'logs/func/%s' % stamp  # <- Name of this `run`\n",
    "writer = tf.summary.create_file_writer(logdir)\n",
    "# Initialization\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "reg_fn = NALURegularizer()\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.01)\n",
    "\n",
    "x = tf.random.normal(shape=(10, 2))\n",
    "y = tf.random.normal(shape=(10, 1))\n",
    "# Start tracing and store it in `tf.summary`\n",
    "tf.summary.trace_on(graph=True, profiler=False)\n",
    "# Code: Begin\n",
    "#############\n",
    "# Call `tf.function` when tracing.\n",
    "\n",
    "tmp = mm.train_step_gating(next(iter(data_dp)))\n",
    "\n",
    "# Code: End\n",
    "###########\n",
    "with writer.as_default():\n",
    "  tf.summary.trace_export(\n",
    "      name=\"train_step_trace\",  # <- Name of tag\n",
    "      step=0,\n",
    "      profiler_outdir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6007 (pid 31811), started 0:00:02 ago. (Use '!kill 31811' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-bdd640fb06671ad1\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-bdd640fb06671ad1\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c6b4ff1917ad7fe489d902c3041fcac2959f3e2431a3acc02677f15f306757aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
