{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-15 16:12:47.711584: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-15 16:12:47.711640: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import ast\n",
    "import abc\n",
    "import random\n",
    "from typing import List, Dict, Union, Tuple\n",
    "import numpy as np\n",
    "from scipy.stats import truncnorm\n",
    "import argparse\n",
    "import timeit\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "# from tensorflow_addons.layers.nalu import NALU\n",
    "%load_ext tensorboard\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\" # disable cuda sepeed up\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1' # disable CPU wornings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-15 16:13:01.007629: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-11-15 16:13:01.007677: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-11-15 16:13:01.007720: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (filip-HP-ProBook-440-G3): /proc/driver/nvidia/version does not exist\n",
      "2022-11-15 16:13:01.009122: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"-o\", \"--output\", dest=\"output\", default=\"naly_syn_simple_arith\")\n",
    "parser.add_argument(\"-d\", \"--dist\", dest=\"dist\", default=\"normal\", help=\"Prob.Dist\")\n",
    "parser.add_argument(\"-p\", \"--params\",dest=\"params\" , default=\"(-3,3)\", type=ast.literal_eval)\n",
    "parser.add_argument(\"-e\", \"--ext\",dest=\"ext\" , default=\"(10,15)\", type=ast.literal_eval)\n",
    "\n",
    "parser.add_argument(\"-n\", \"--nalu\", dest=\"nalu\", default=\"nalui1\")\n",
    "parser.add_argument(\"-se\", \"--seed\", dest=\"seed\", default=42, type=int)\n",
    "parser.add_argument(\"-op\", \"--operation\", dest=\"op\", default=\"MUL\")\n",
    "\n",
    "\n",
    "\n",
    "args = parser.parse_args([])\n",
    "\n",
    "def operation(op, a, b):\n",
    "    if op.lower() == \"mul\":\n",
    "        return a * b\n",
    "    if op.lower() == \"add\":\n",
    "        return a + b\n",
    "    if op.lower() == \"sub\":\n",
    "        return a - b\n",
    "    if op.lower() == \"div\":\n",
    "        return a / b\n",
    "\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "w, m, g = np.load(\"w.npy\"), np.load(\"m.npy\"), np.load(\"g.npy\")\n",
    "data, int_data, ext_data = np.load(\"data.npy\"), np.load(\"int_data.npy\"), np.load(\"ext_data.npy\")\n",
    "lbls = operation(args.op, data[:,0], data[:,1])\n",
    "lbls = np.reshape(lbls, newshape=(-1, 1))\n",
    "\n",
    "int_lbls = operation(args.op, int_data[:,0], int_data[:,1])\n",
    "int_lbls = np.reshape(int_lbls, newshape=(-1, 1))\n",
    "\n",
    "\n",
    "ext_lbls = operation(args.op, ext_data[:,0], ext_data[:,1])\n",
    "ext_lbls = np.reshape(ext_lbls, newshape=(-1, 1))\n",
    "\n",
    "losses = np.load(\"losses.npy\")\n",
    "rlosses = np.load(\"rlosses.npy\")\n",
    "ext_loss = np.load(\"ext_loss.npy\")\n",
    "int_loss = np.load(\"int_loss.npy\")\n",
    "rext_loss = np.load(\"rext_loss.npy\")\n",
    "rint_loss = np.load(\"rint_loss.npy\")\n",
    "\n",
    "\n",
    "data_dp = tf.data.Dataset.from_tensor_slices((data, lbls)).prefetch(tf.data.AUTOTUNE).batch(BATCH_SIZE)\n",
    "int_data_dp = tf.data.Dataset.from_tensor_slices((int_data, int_lbls)).prefetch(tf.data.AUTOTUNE) #.batch(BATCH_SIZE)\n",
    "ext_data_dp = tf.data.Dataset.from_tensor_slices((ext_data, ext_lbls)).prefetch(tf.data.AUTOTUNE) #.batch(BATCH_SIZE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =====================================================epoch\n",
    "import tensorflow as tf\n",
    "from typeguard import typechecked\n",
    "from typing import List\n",
    "from tensorflow_addons.utils import types\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "\n",
    "class NALURegularizer(tf.keras.regularizers.Regularizer):\n",
    "    def __init__(self, reg_coef=0.1):\n",
    "        self.reg_coef = reg_coef\n",
    "\n",
    "    def __call__(self, var: List[tf.Variable]) -> tf.Tensor:\n",
    "        return self.reg_coef * tf.add_n(\n",
    "            [\n",
    "                tf.reduce_mean(tf.math.maximum(tf.math.minimum(-v, v) + 20, 0))\n",
    "                for v in var\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\"reg_coef\": float(self.reg_coef)}\n",
    "\n",
    "\n",
    "# @tf.keras.utils.register_keras_serializable(package=\"Addons\")\n",
    "class NALU(tf.keras.layers.Layer):\n",
    "    r\"\"\"Neural Arithmetic Logic Units\n",
    "    A layer that learns addition,substraction, multiplication and division\n",
    "    in transparent way. They layer has two paths: one for addition/substration\n",
    "    and one for multiplication/division. We can inspect weights for these two\n",
    "    paths by calling `w_hat` and `m_hat` respectively. To use this layer reliably,\n",
    "    we have to delay regularization of gating varaible that switch between two paths.\n",
    "    Ithave to be done by callback as from the layer-level we keep no information about epochs.\n",
    "    See [Neural Arithmetic Logic Units](https://arxiv.org/abs/1808.00508)\n",
    "    and [Improved Neural Arithmetic Logic Unit](https://arxiv.org/abs/2003.07629)\n",
    "    Example:\n",
    "    >>> BATCH_SIZE, INPUT_SIZE, OUTPUT_SIZE = 16, 5, 2\n",
    "    >>> input = tf.random.uniform((BATCH_SIZE, INPUT_SIZE))\n",
    "    >>> nalu_layer = NALU(OUTPUT_SIZE)\n",
    "    >>> predict = nalu_layer(input)\n",
    "    >>> assert predict.shape == (BATCH_SIZE, OUTPUT_SIZE)\n",
    "    Args:\n",
    "        input_dim (int): input\n",
    "        output_dim (int): _description_\n",
    "        regularizer (types.Regularizer, optional): _description_. Defaults to NALURegularizer(reg_coef=0.05).\n",
    "        gate_as_vector (bool, optional): _description_. Defaults to True.\n",
    "        clipping (float, optional): _description_. Defaults to None.\n",
    "        force_operation (str, optional): _description_. Defaults to None.\n",
    "        weights_separation (bool, optional): _description_. Defaults to True.\n",
    "        input_gate_dependance (bool, optional): _description_. Defaults to True.\n",
    "        w_initializer (types.Initializer, optional): _description_. Defaults to tf.random_normal_initializer( mean=0.88, stddev=0.2, seed=None ).\n",
    "        m_initializer (types.Initializer, optional): _description_. Defaults to tf.random_normal_initializer( mean=0.5, stddev=0.2, seed=None ).\n",
    "        g_initializer (types.Initializer, optional): _description_. Defaults to tf.random_normal_initializer( mean=0.0, stddev=0.2, seed=None ).\n",
    "    \"\"\"\n",
    "\n",
    "    class LoopStep(tf.keras.Model):\n",
    "        \"\"\"It's super class to create models with NALU layer which controls the training step.\n",
    "        To speed up we use @tf.function to build graph of computation, but dynamically control\n",
    "        myst rely on tf.variables as all other attributes are compiled to static values.\n",
    "        E.g. self.regularize controls when to start regularization and self.gating controls\n",
    "        when to train gates insted of active varaivles. We can dynamically reassign values \n",
    "        to these variables to controll training steps from the callback positions (LoopControll).\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, *args, **kwargs):\n",
    "            \n",
    "            super(NALU.LoopStep, self).__init__(*args, **kwargs)\n",
    "            self.regularize = tf.Variable(False, trainable=False)\n",
    "            self.gating = tf.Variable(False, trainable=False)\n",
    "            self.gate_var = None\n",
    "\n",
    "        def reinitialise(self):\n",
    "            for l in self.layers:\n",
    "                if isinstance(l, NALU):\n",
    "                    l.reinitialise()\n",
    "\n",
    "        def get_gates_variables(self) -> List[tf.Variable]:\n",
    "            return [l.g for l in self.layers if isinstance(l, NALU)]\n",
    "\n",
    "        def get_regularization_loss(self):\n",
    "            return tf.math.reduce_sum(self.losses)\n",
    "\n",
    "        # @tf.function\n",
    "        def train_step_active(self, data):\n",
    "            if not self.gate_var:\n",
    "                self.gate_var = self.get_gates_variables()\n",
    "            x, y = data\n",
    "            with tf.GradientTape() as tape:\n",
    "                logits = self(x, training=True)\n",
    "                loss_value = self.compiled_loss(y, logits)\n",
    "            grads = tape.gradient(loss_value, tape.watched_variables())\n",
    "            grads = [tf.clip_by_value(g, -0.1, 0.1) for g in grads]\n",
    "            self.optimizer.apply_gradients(zip(grads, tape.watched_variables()))\n",
    "            return {**{m.name: m.result() for m in self.metrics}, **{\"ll\": loss_value}}\n",
    "\n",
    "        # @tf.function\n",
    "        def train_step_gating(self, data):\n",
    "            if not self.gate_var:\n",
    "                self.gate_var = self.get_gates_variables()\n",
    "            x, y = data\n",
    "            with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "                for g in self.gate_var:\n",
    "                    tape.watch(g)\n",
    "\n",
    "                logits = self(x, training=True)\n",
    "                loss_value = self.compiled_loss(y, logits)\n",
    "\n",
    "            grads = tape.gradient(loss_value, tape.watched_variables())\n",
    "            grads = [tf.clip_by_value(g, -0.1, 0.1) for g in grads]\n",
    "            self.optimizer.apply_gradients(zip(grads, tape.watched_variables()))\n",
    "            return {**{m.name: m.result() for m in self.metrics}, **{\"ll\": loss_value}}\n",
    "\n",
    "        # @tf.function\n",
    "        def train_step_regularize(self):\n",
    "            with tf.GradientTape(watch_accessed_variables=True) as tape:\n",
    "                for g in self.gate_var:\n",
    "                    tape.watch(g)\n",
    "                reg_loss = self.get_regularization_loss()\n",
    "            grads = tape.gradient(reg_loss, tape.watched_variables())\n",
    "            grads = [tf.clip_by_value(g, -0.1, 0.1) for g in grads]\n",
    "            self.optimizer.apply_gradients(zip(grads, tape.watched_variables()))\n",
    "            return reg_loss\n",
    "\n",
    "        # @tf.function\n",
    "        def train_step(self, data):\n",
    "            \"\"\"\n",
    "            Specifying tf.function(input_signature=...) slows down the computation, but it leads to greater control:\n",
    "            https://www.neuralconcept.com/post/in-graph-training-loop\n",
    "            \"\"\"\n",
    "            #print(\"train step - print\")\n",
    "            metrics_train = tf.cond(\n",
    "                self.gating,\n",
    "                lambda: self.train_step_gating(data),\n",
    "                lambda: self.train_step_active(data),\n",
    "            )\n",
    "\n",
    "            metrics_train[\"rl\"] = tf.cond(\n",
    "                self.regularize, lambda: self.train_step_regularize(), lambda: 0.0\n",
    "            )\n",
    "            return metrics_train\n",
    "\n",
    "        # @tf.function\n",
    "        def predict_loss(self, x, y):\n",
    "            logits = self(x, training=False)\n",
    "            return self.compiled_loss(y, logits)\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "    class LoopControll(tf.keras.callbacks.Callback):\n",
    "        \"\"\"Callback that controll the training loop. It controlls variables of the model that subclass LoopStep so that\n",
    "        we controll how the trainign loop work still usingn default model.fit(...) functionality. We modify LoopSep-model\n",
    "        variables values via accessing self.model.__controll_variables__. Using [callback](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback) \n",
    "        functions we controll variables values reasignment from the training-loop level. \n",
    "        Args:\n",
    "            ext_data (types.TensorLike): data to check performance on extrapolation data\n",
    "            ext_label (types.TensorLike): labels to check performance on extrapolation data\n",
    "            int_data (types.TensorLike): data to check performance on intrapolation data\n",
    "            int_label (types.TensorLike): labels to check performance on extrapolation data\n",
    "            regularization_delay (int, optional): number of epochs after which regularization starts (if loss is smaller thatn regularization_loss_threshold). Defaults to 8.\n",
    "            regularization_loss_threshold (float, optional): loss threshold below which regularization starts (if number of epochs is greater than regularization_delay). Defaults to 1.0.\n",
    "            param_check (int, optional): number of steps after which performance on extra/inter-polation data is calculated. Results can be accessed by ext-res and int-res, respectively. Defaults to 10000.\n",
    "        \"\"\"\n",
    "\n",
    "        @typechecked\n",
    "        def __init__(\n",
    "            self,\n",
    "            ext_data: types.TensorLike,\n",
    "            ext_label: types.TensorLike,\n",
    "            int_data: types.TensorLike,\n",
    "            int_label: types.TensorLike,\n",
    "            regularization_delay: int = 8,\n",
    "            regularization_loss_threshold: float = 1.0,\n",
    "            param_check: int = 10000,\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        ):\n",
    "\n",
    "\n",
    "            super(NALU.LoopControll, self).__init__(*args, **kwargs)\n",
    "            NALU.LoopControll.regularization_delay = regularization_delay\n",
    "            NALU.LoopControll.regularization_loss_threshold = regularization_loss_threshold\n",
    "            NALU.LoopControll.param_check = param_check\n",
    "            self.ext_data = ext_data\n",
    "            self.ext_label = ext_label\n",
    "            self.int_data = int_data\n",
    "            self.int_label = int_label\n",
    "            # self.param_check_verbose = param_check_verbose\n",
    "\n",
    "        def on_train_begin(self, logs=None):\n",
    "            NALU.LoopControll.reinit_history = []\n",
    "            NALU.LoopControll.reinit_counter = 0\n",
    "            NALU.LoopControll._steps_counter = 0\n",
    "            NALU.LoopControll._epoch_counter = 0\n",
    "            NALU.LoopControll.ext_res = []\n",
    "            NALU.LoopControll.int_res = []\n",
    "\n",
    "            NALU.LoopControll.gate_counter = 0\n",
    "            NALU.LoopControll.gate_history = []\n",
    "\n",
    "        # delay regularize\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            NALU.LoopControll._epoch_counter += 1\n",
    "\n",
    "        def on_train_batch_end(self, batch, logs=None):\n",
    "\n",
    "            NALU.LoopControll._steps_counter += 1\n",
    "\n",
    "            # record last loss\n",
    "            NALU.LoopControll.reinit_history.append(logs.get(\"loss\"))\n",
    "\n",
    "            # train either active or gating\n",
    "            self.model.gating.assign(NALU.LoopControll._steps_counter % 10 > 8)\n",
    "            NALU.LoopControll.gate_history.append(self.model.gating.numpy())\n",
    "            if NALU.LoopControll._steps_counter % 10 > 8:\n",
    "                NALU.LoopControll.gate_counter += 1\n",
    "\n",
    "            # turn on or of ragularization depending on epoch number and last seen loss\n",
    "\n",
    "            self.model.regularize.assign(\n",
    "                NALU.LoopControll._epoch_counter > NALU.LoopControll.regularization_delay\n",
    "                and NALU.LoopControll.reinit_history[-1] < NALU.LoopControll.regularization_loss_threshold\n",
    "            )\n",
    "\n",
    "            # reinitialisation strategy\n",
    "            split_index = len(NALU.LoopControll.reinit_history) // 2\n",
    "            if (\n",
    "                len(NALU.LoopControll.reinit_history) > 10000\n",
    "                and NALU.LoopControll._epoch_counter > 0\n",
    "                and NALU.LoopControll._epoch_counter % 10 == 1\n",
    "                and tf.math.reduce_mean(NALU.LoopControll.reinit_history[:split_index])\n",
    "                <= tf.math.add(\n",
    "                    tf.math.reduce_mean(NALU.LoopControll.reinit_history[split_index:]),\n",
    "                    tf.math.reduce_std(NALU.LoopControll.reinit_history[split_index:]),\n",
    "                )\n",
    "                and tf.math.reduce_mean(NALU.LoopControll.reinit_history[split_index:])\n",
    "                > 1\n",
    "            ):\n",
    "                # reinitialize all nalu layers\n",
    "                self.model.reinitialise()\n",
    "                NALU.LoopControll.reinit_history = []\n",
    "                NALU.LoopControll.reinit_counter += 1\n",
    "\n",
    "            # check parameters\n",
    "            if self._steps_counter % NALU.LoopControll.param_check == 0:\n",
    "                eloss_ex = self.model.compiled_loss._losses[0](\n",
    "                    self.model.predict(self.ext_data, verbose=0), self.ext_label\n",
    "                )\n",
    "                eloss_in = self.model.compiled_loss._losses[0](\n",
    "                    self.model.predict(self.int_data, verbose=0), self.int_label\n",
    "                )\n",
    "                NALU.LoopControll.ext_res.append(eloss_ex)\n",
    "                NALU.LoopControll.int_res.append(eloss_in)\n",
    "\n",
    "    @typechecked\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        output_dim: int,\n",
    "        regularizer: types.Regularizer = NALURegularizer(reg_coef=0.05),\n",
    "        gate_as_vector: bool = True,\n",
    "        clipping: float = 20,\n",
    "        force_operation: str = None,\n",
    "        weights_separation: bool = True,\n",
    "        input_gate_dependance: bool = True,\n",
    "        w_initializer: types.Initializer = tf.random_normal_initializer(\n",
    "            mean=1.0, stddev=0.1, seed=None\n",
    "        ),\n",
    "        m_initializer: types.Initializer = tf.random_normal_initializer(\n",
    "            mean=-1.0, stddev=0.1, seed=None\n",
    "        ),\n",
    "        g_initializer: types.Initializer = tf.random_normal_initializer(\n",
    "            mean=0.0, stddev=0.1, seed=None\n",
    "        ),\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(NALU, self).__init__(*args, **kwargs)\n",
    "\n",
    "        self.reg_fn = regularizer\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.gate_as_vector = gate_as_vector\n",
    "        self.clipping = clipping\n",
    "        self.force_operation = force_operation\n",
    "        self.weights_separation = weights_separation\n",
    "        self.input_gate_dependance = input_gate_dependance\n",
    "        self.w_initializer = w_initializer\n",
    "        self.m_initializer = m_initializer\n",
    "        self.g_initializer = g_initializer\n",
    "\n",
    "        self.operator_weight_shape = (self.input_dim, self.output_dim) if self.gate_as_vector else (self.output_dim, self.input_dim)\n",
    "        self.gate_shape = (self.input_dim, self.output_dim if self.gate_as_vector else 1) if self.input_gate_dependance else (self.output_dim,)\n",
    "\n",
    "        # action variables\n",
    "        self.w_hat = self.add_weight (\n",
    "            shape = self.operator_weight_shape,\n",
    "            initializer = self.w_initializer,\n",
    "            trainable = True,\n",
    "            name = \"w\",\n",
    "            use_resource = False\n",
    "        )\n",
    "\n",
    "        self.m_hat = self.add_weight (\n",
    "            shape = self.operator_weight_shape,\n",
    "            initializer = self.m_initializer,\n",
    "            trainable = True,\n",
    "            name = \"m\",\n",
    "            use_resource = False\n",
    "        )\n",
    "\n",
    "        # gating varaible\n",
    "        self.g = self.add_weight (\n",
    "            shape = self.gate_shape,\n",
    "            initializer = self.g_initializer,\n",
    "            trainable = False,\n",
    "            name = \"g\",\n",
    "            use_resource = False\n",
    "        )\n",
    "\n",
    "        self.g = tf.Variable(\n",
    "            self.g_initializer(shape=self.gate_shape), trainable=False, name=\"g\"\n",
    "        )\n",
    "\n",
    "        self.w_hat_prime = tf.Variable(\n",
    "                self.w_initializer(shape=self.operator_weight_shape),\n",
    "                trainable=True,\n",
    "                name=\"wprime\",\n",
    "            ) if self.weights_separation else None\n",
    "        \n",
    "        self.m_hat_prime = tf.Variable(\n",
    "                self.m_initializer(shape=self.operator_weight_shape),\n",
    "                trainable=True,\n",
    "                name=\"mprime\",\n",
    "            ) if self.weights_separation else None\n",
    "        \n",
    "    # @tf.function\n",
    "    def get_reg_loss(self):\n",
    "        var_list = [self.w_hat, self.m_hat, self.g]\n",
    "        if self.weights_separation:\n",
    "            var_list += [self.w_hat_prime, self.m_hat_prime]\n",
    "        return self.reg_fn(var_list)\n",
    "\n",
    "    def __call__(self, input):\n",
    "        eps = 1e-7\n",
    "        w1 = tf.math.tanh(self.w_hat) * tf.math.sigmoid(self.m_hat)\n",
    "        w2 = tf.math.tanh(self.w_hat_prime) * tf.math.sigmoid(self.m_hat_prime)\n",
    "        a1 = tf.matmul(input, w1)\n",
    "\n",
    "        m1 = tf.math.exp(tf.minimum(tf.matmul(tf.math.log(tf.maximum(tf.math.abs(input), eps)),w2), self.clipping))\n",
    "        \n",
    "        # sign\n",
    "        w1s = tf.math.abs(tf.reshape(w2, [-1]))\n",
    "        xs = tf.concat([input] * w1.shape[1], axis=1)\n",
    "        xs = tf.reshape(xs, shape=[-1, w1.shape[0] * w1.shape[1]])\n",
    "        sgn = tf.sign(xs) * w1s + (1 - w1s)\n",
    "        sgn = tf.reshape(sgn, shape=[-1, w1.shape[1], w1.shape[0]])\n",
    "        ms = tf.math.reduce_prod(sgn, axis=2)\n",
    "        \n",
    "        self.add_loss(lambda: self.get_reg_loss())\n",
    "\n",
    "        g1 = tf.math.sigmoid(self.g)\n",
    "        return g1 * a1 + (1 - g1) * m1 * tf.clip_by_value(ms, -1, 1)\n",
    "\n",
    "    def reinitialise(self):\n",
    "        self.g.assign(tf.random.uniform(self.g.shape, -2, 2))\n",
    "        self.w_hat.assign(tf.random.uniform(self.w_hat.shape, -2, 2))\n",
    "        self.m_hat.assign(tf.random.uniform(self.m_hat.shape, -2, 2))\n",
    "        if self.weights_separation:\n",
    "            self.w_hat_prime.assign(tf.random.uniform(self.w_hat_prime.shape, -2, 2))\n",
    "            self.m_hat_prime.assign(tf.random.uniform(self.m_hat_prime.shape, -2, 2))\n",
    "\n",
    "    def get_gates_variables(self) -> List[tf.Variable]:\n",
    "        return [self.g]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.keras.backend.set_learning_phase(0)\n",
    "\n",
    "class NALUModel(NALU.LoopStep):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(NALUModel, self).__init__(*args, **kwargs)\n",
    "\n",
    "        self.layer1 = NALU(2, 1,\n",
    "            gate_as_vector = True,\n",
    "            input_gate_dependance = False,\n",
    "            clipping = 20\n",
    "            )\n",
    "        \n",
    "    def call(self, inputs: tf.Tensor, training: bool = True):\n",
    "        return self.layer1(inputs)\n",
    "\n",
    "# tf.keras.backend.clear_session()\n",
    "model = NALUModel()\n",
    "\n",
    "model.compile(optimizer=\n",
    "    tf.keras.optimizers.RMSprop(learning_rate=0.01), #, epsilon = 1e-10), \n",
    "    loss=\"mse\", \n",
    "    metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 10s 4ms/step - loss: 0.3159 - ll: 0.3156 - rl: 0.0000e+00\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.0373 - ll: 0.0373 - rl: 0.0000e+00\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.0090 - ll: 0.0090 - rl: 0.0000e+00\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.0020 - ll: 0.0020 - rl: 0.0000e+00\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 4.2487e-04 - ll: 4.2459e-04 - rl: 0.0000e+00\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 8.8484e-05 - ll: 8.8424e-05 - rl: 0.0000e+00\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 1.8270e-05 - ll: 1.8258e-05 - rl: 0.0000e+00\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 3.1043e-07 - ll: 3.1012e-07 - rl: 7.4983\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 4.8177e-15 - ll: 4.8160e-15 - rl: 0.4997\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 4.1423e-15 - ll: 4.1413e-15 - rl: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(data_dp, batch_size = 64, epochs = 10, callbacks=[\n",
    "    NALU.LoopControll(ext_data, ext_lbls, int_data, int_lbls, regularization_delay = 6, param_check = 10000)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(len(NALU.LoopControll.reinit_history)), NALU.LoopControll.reinit_history)\n",
    "plt.ylabel(\"MSE loss [1]\")\n",
    "plt.xlabel(\"Training step [1]\")\n",
    "plt.title(\"Loss in tf.fit wrapper\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "99b15cb6107b0b495b17a85b55d21f2ba642084aff95f5e6b9621ef13b3a13bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
